,answer,author,question,topic
0,"You’re asking the exact same question I was asking myself about a year ago. I was working at the Apple Store and I wanted a change. I wanted to start building the tech I was servicing.
I started looking into Machine Learning (ML) and Artificial Intelligence (AI).
There’s so much going on in the field.
Every week it seems like Google or Facebook are releasing a new kind of AI to make things faster or improve our experience.
Don’t get me started on the number of self-driving car companies. This is a good thing though. I’m not a fan of driving and roads are dangerous.
Even with all this happening, there’s still yet to be an agreed definition of what exactly artificial intelligence is.
Some argue deep learning can be considered AI, others will say it’s not AI unless it passes the Turing Test.
This lack of definition really stunted my progress in the beginning. It was hard to learn something which had so many different definitions.
Enough with the definitions.
How did I get started?
My friends and I were building a web startup. It failed. We gave up due to a lack of meaning. But along the way, I was starting to hearing more and more about ML and AI.
“The computer learns the things for you?” I couldn’t believe it.
I stumbled across Udacity’s Deep Learning Nanodegree. A fun character called Siraj Raval was in one of the promo videos. His energy was contagious. Despite not meeting the basic requirements (I had never written a line of Python before), I signed up.
Three weeks before the course start date I emailed Udacity support asking what the refund policy was. I was scared I wouldn’t be able to complete the course.
I didn’t get a refund. I completed the course within the designated timeline. It was hard. Really hard at times. My first two projects were handed in four days late. But the excitement of being involved in one of the most important technologies in the world drove me forward.
Finishing the Deep Learning Nanodegree, I had guaranteed acceptance into either Udacity’s AI Nanodegree, Self-Driving Car Nanodegree or Robotics Nanodegree. All great options. I was a little lost. “Where do I go next?”
I needed a curriculum. I’d built a little foundation with the Deep Learning Nanodegree, now it was time to figure out where I’d head next.
My Self-Created AI Masters Degree[1]
I didn’t plan on going back to university anytime soon. I didn’t have $100,000 for a proper Masters Degree anyway.
So I did what I did in the beginning. Asked my mentor, Google, for help.
I’d jumped into deep learning without any prior knowledge of the field. Instead of climbing to the tip of the AI iceberg, a helicopter had dropped me off on the top.
After researching a bunch of courses, I put a list of which ones interested me the most in Trello.
Trello is my personal assistant/course coordinator.
I knew online courses had a high drop out rate. I wasn’t going to let myself be a part of this number. I had a mission.
To make myself accountable, I started sharing my learning journey online. I figured I could practice communicating what I learned plus find other people who were interested in the same things I was. My friends still think I’m an alien when I go on one of my AI escapades.
I made the Trello board public[2]and wrote a blog post about my endeavours.
The curriculum has changed slightly since I first wrote it but it’s still relevant and I visit the Trello board multiple times per week to track my progress.
Getting a job
I bought a plane ticket to the US with no return flight. I’d been studying for a year and I figured it was about time I started putting my skills into practice.
My plan was to rock up to the US and get hired.
Then Ashlee messaged me on LinkedIn, “Hey I’ve seen your posts and they’re really cool, I think you should meet Mike.”
I met Mike.
I told him my story of learning online, how I loved healthtech and my plans to go to the US.
“You may be better off staying here a year or so and seeing what you can find, I’ think you’d love to meet Cameron.”
I met Cameron.
We had a similar chat what Mike and I talked about. Health, tech, online learning, US.
“We’re working on some health problems, why don’t you come in on Thursday?”
Thursday came. I was nervous. But someone once told me being nervous is the same as being excited. I flipped to being excited.
I spent the day meeting the Max Kelsen team and the problems they were working on.
Two Thursday’s later, Nick, the CEO, Athon, lead machine learning engineer, and I went for coffee.
“How would you like to join the team?” Asked Nick.
“Sure.” I said.[3]
My US flight got pushed back a couple months, I’ve also got a return ticket.
Sharing your work
Learning online, I knew it was unconventional. All the roles I’d gone to apply for had Masters Degree requirements or at least some kind of technical degree.
I didn’t have either of these. But I did have the skills I’d gathered from a plethora of online courses.
Along the way, I was sharing my work online. My GitHub contained all the projects I’d done, my LinkedIn was stacked out and I’d practiced communicating what I learned through YouTube and articles on Medium.
I never handed in a resume for Max Kelsen. “We checked you out on LinkedIn and we were impressed.” My body of work was my resume.
Regardless if you’re learning online or through a Masters Degree, having a portfolio of what you’ve worked on is a great way to build skin in the game.
ML and AI skills are in demand but that doesn’t mean you don’t have to showcase them. Even the best product won’t sell without any shelf space.
Whether it be GitHub, Kaggle, LinkedIn or a blog, have somewhere where people can find you. Plus, having your own corner of the internet is great fun.
How do you start?
Where do you go to learn these skills? What courses are the best?
There’s no best answer. Everyone’s path will be different. Some people learn better with books, others learn better through videos.
What’s more important than how you start is why you start.
Start with why.
Why do you want to learn these skills?
Do you want to make money?
Do you want to build things?
Do you want to make a difference?
Again, no right reason. All are valid in their own way.
Start with why because having a why is more important than how. Having a why means when it gets hard and it will get hard, you’ve got something to turn to. Something to remind you why you started.
Got a why? Good. Time for some hard skills.
I can only recommend what I’ve tried.
I’ve completed courses from (in order):
Treehouse - Introduction to Python
Udacity - Deep Learning & AI Nanodegree
Coursera - Deep Learning by Andrew Ng
fast.ai - Part 1, soon to be Part 2
They’re all world class. I’m a visual learner. I learn better seeing things being done/explained to me on. So all of these courses reflect that.
If you’re an absolute beginner, start with some introductory Python courses and when you’re a bit more confident, move into data science, machine learning and AI.
How much math?
The highest level of math education I’ve had was in high school. The rest I’ve learned through Khan Academy as I’ve needed it.
There are many different opinions on how much math you need to know to get into machine learning and AI. I’ll share mine.
If you want to apply machine learning and AI techniques to a problem, you don’t necessarily need an in-depth understanding of the math to get a good result. Libraries such as TensorFlow and PyTorch allow someone with a bit of Python experience to build state of the art models whilst the math is taken care of behind the scenes.
If you’re looking to get deep into machine learning and AI research, through means of a PhD program or something similar, having an in-depth knowledge of the math is paramount.
In my case, I’m not looking to dive deep into the math and improve an algorithm’s performance by 10%. I’ll leave that to people smarter than me. Instead, I’m more than happy to use the libraries available to me and manipulate them to help solve problems as I see fit.
What does a machine learning engineer actually do?
What a machine engineer does in practice might not be what you think.
Despite the cover photos of many online articles, it doesn’t always involve working with robots that have red eyes.
Here are a few questions an ML engineer has to ask themselves daily.
Context - How can ML be used to help learn more about your problem?
Data - Do you need more data? What form does it need to be in? What do you do when data is missing?
Modeling - Which model should you use? Does it work too well on the data (overfitting)? Or why doesn’t it work very well (underfitting)?
Production - How can you take your model to production? Should it be an online model or should it be updated at time intervals?
Ongoing - What happens if your model breaks? How do you improve it with more data? Is there a better way of doing things?
I borrowed these from a great article by Rachel Thomas, one of the co-founders of fast.ai, she goes into more depth in the full text. [4]
For more, I made a video of what we usually get up to on Monday’s at Max Kelsen[5].
No set path
There’s no right or wrong way to get into ML or AI.
The beautiful thing about this field is we have access to some of the best technologies in the world, all we’ve got to do is learn how to use them.
You could begin by learning Python code.
You could begin by studying calculus and statistics.
You could begin by learning about the philosophy of decision making.
Machine learning and AI fascinates me because of this intersection of fields.
The more I learn about it, the more I realise there’s plenty more to learn. And this hypes me up.
Sometimes I get frustrated when my code doesn’t run. Or I don’t understand a concept. So I give up temporarily. I give up by letting myself walk away from the problem and take a nap. Or go for a walk. When I come back it feels like I’m looking at it with different eyes. The excitement comes back. I keep learning.
There’s so much happening in the field it can be daunting to get started. Too many options lead to no options. Ignore this.
Start wherever interests you most and follow it. If it leads to a dead end, great, you’ve figured out what you’re not interested in. Retrace your steps and take the other fork in the road instead.
Computers are smart but they still can’t learn on their own. They need your help.
Footnotes
[1] My Self-Created Artificial Intelligence Masters Degree
[2] Trello
[3] My first day as a MACHINE LEARNING Engineer | Learning Intelligence 32
[4] What do machine learning practitioners actually do?
[5] Max Kelsen | Big data. Big ideas
189.5k Views · View 5k Upvoters · View Sharers",Daniel Bourke,I want to learn Artificial Intelligence and Machine learning. Where can I start?,Artificial Intelligence
1,"Originally Answered: Andrew Ng: What triggered your desire to learn artificial intelligence?
I want us to build a better society using AI.
Just as the industrial revolution relieved humanity of a lot of physical drudgery (what would your life be like if you had to sew your own clothes?), in the future AI will relieve humanity of mental drudgery. For example, having autonomous cars means we will no longer have to waste 3 years of our lives driving. This will give us more time to spend with loved ones and to pursue more worthy goals!
I'd started implementing neural networks (now called ""deep learning"") when I was 16 years old. I was an intern at the National University of Singapore. What we did then was trivial compared to today's standards, but at the time I thought it was amazing that a few lines of code could cause a computer to learn!
Nerdy fun fact: Because the computers we were using were too slow for emacs, I learned vi, which still remains my preferred editor today.
Around that time, I also did an internship where I was an office assistant, and did a lot of photocopying. (Technically, I was the office assistant's assistant.) That's when I realized that smarter machine can help us do a lot of tasks, so that humans can do more intellectually challenging things.
In the next decade, AI will transform society. It will change what we do vs. what we get computers to do for us. Perhaps a few decades from now, there'll be a Quora post where someone asks ""what would your life be like if you had to drive your own car?"" 
151.5k Views · View 3.9k Upvoters · View Sharers",Andrew Ng,What triggered Andrew Ng's desire to learn artificial intelligence?,Artificial Intelligence
2,"The choice of a Gaussian can be justified by the principle of maximum entropy, which suggests that if the true distribution is unknown, one should use the distribution with the greatest entropy among those which satisfy whatever constraints we wish to impose. This essentially makes the fewest possible assumptions about the distribution, as the max entropy distribution is the least predictable.
Often the constraints we impose are matching moments. For example, if you have some data and can use it to estimate the mean and variance of your unknown distribution, you might seek the distribution with greatest entropy such that its mean and variance match your empirical mean and variance. Any guesses what this distribution turns out to be? Ding ding ding! It’s a Gaussian.
Another reason (which is not so much a theoretical justification as a practical motivation) is that Gaussian distributions have many nice algebraic properties, for example that the sum of independent Gaussians is Gaussian, and the marginal and conditional distributions of multivariate Gaussians are Gaussian. These properties sometimes allow us to derive closed form solutions for certain quantities, making them computable without resorting to approximations.
Another answer mentions the Central Limit Theorem. This provides justification if you believe your unknown distribution involves a sum/average of many independent (or weakly dependent) sources of error. This is approximately true in many natural scenarios but I imagine not the reason for choosing a Gaussian in all cases.
3.6k Views · View 55 Upvoters · View Sharers",Garrett Thomas,Why is it reasonable in Machine Learning (e.g. Generative Models in Deep Learning) to describe unknown distributions as Gaussian?,Artificial Intelligence
3,"Human extinction.
972 Views · View 100 Upvoters · Answer requested by
Darren Blackburn",David Seidman,What is the next big thing after AI?,Artificial Intelligence
4,"Randall closed his eyes.
The sun beamed high from above, dead center in the middle of the sky, with only a few meager clouds on the horizon. The front lawns of the suburbs graced the Earth, stretching outward for miles.
Every day, it seemed, the grass grew just a little bit taller. Some yards grew just a little more yellow. Some houses darkened as the weeks went on.
With time, the presence of a moving van seemed to change from an anomaly to a permanent fixture. A sign that shouted ‘foreclosure’. An ever-present ferryman, always prepared to move today’s family — some days, more than one— along with their belongings deep into the warm glow of the cities, into the comfort of affordable living, and away from the growing cold of the ‘burbs.
The ‘burbs. It all happened so suddenly for him. It certainly didn’t feel like years.
Every day, it seemed, another house’s lights were snuffed out. The ‘burbs; growing colder and colder and colder with every passing week, as the very Earth itself seemed to slowly extinguish. The cold and desolate suburbs, growing colder still, until its chill touched the edge of the city’s borders. It clawed away with an icy touch, clawing with desperation at the people held within the city’s vehement clutches, each huddled together en mass to stave off the chill. Hundreds of thousands huddled close together until every room in every skyscraper was filled beyond capacity. A warm glow of togetherness somehow remained intact, despite the threatening cold from the world outside.
He had memories of an automated food delivery truck being attacked. Hungry people gathering around, throwing wire traps, rocks, metal rods — until the wheels of the behemoth ground to a halt. Memories of police officers struggling to maintain order, as food was pulled out from its iron stomach. Packaged bread. Meats. Fruit. Fruit, damn it. Fresh and bright, like they were Christmas decorations.
Food autos stopped passing through the city soon after that. Eventually, without anybody wealthy enough or working enough to purchase their wares, food autos disappeared altogether.
He had memories of images on screens. Protests. Riots. People fighting for the privilege to live another day.
A fair deal of them took place in countries that didn’t so much need their citizens anymore. Citizens that couldn’t work couldn’t pay tax, and citizens that didn’t pay tax weren’t as valuable in the eyes of a starving government.
He had memories of posters and signs and megaphones and shouting in strange languages, all quickly giving way to automatic gunfire and a screaming that was universally recognized. Each one dispersing, fluttering away like leaves in the wind, save for those few limp bodies that remained face-down in the mud, alongside whoever had stubbornly remained with them.
He remembered the first time he found somebody unfortunate enough to be lying face-down in the ground. Right in the middle of the sidewalk. Not in a faraway another country seen only through a screen. No pulse. No breaths. He called out for help. For emergency services. For anything. The man’s remains were adorned with torn rags. His elbows were wide, his neck was thin, his rib-cage showed through the holes in his shirt. A human police officer arrived, but his body was wheeled away in an automated emergency vehicle.
He remembered the first time he saw such an individual alight in the nighttime. Nowhere safe to move his remains. Hospitals were either overcrowded or defunct by then. He remembered watching through the curtains as the kerosene was poured almost reverently over the still corpse. He remembered seeing the match strike, like a glowing star in the night sky, before it fell to the road. He remembered the plume of smoke floating up above the skyscrapers, as automated security robots formed a perimeter around the pragmatic cremation.
He remembered receiving messages from family and friends. Proclaiming their hopes to emigrate into a kinder world, somewhere in another country.
For whatever reason, he had chosen to stay.
Perhaps he had a sense of pride for his country. Growing up, he always heard of people running away after a particularly bad election, rather than choosing to stay and fight back. Whenever he had considered such an action, his mind always wandered back to the ballot; they’d need his vote in particular this next election, he always told himself.
Or perhaps he believed that it was simply inevitable, no matter where you went. Countries worldwide had begun to topple, one after the other. With the computers taking over, the unemployment rate rose and never stopped rising. With fewer people employed, fewer people consumed, until massive stockpiles of goods stood patiently waiting for nobody to buy them. The robots producing forevermore — diligent little workers that demanded only electricity to continue their maddening endeavor of adding to the endless growing pile.
Without taxpayer’s revenue, governments slowed to a halt. Fearing for their own stability, tariffs were enacted, degenerating trade. No matter where you went, Randall reasoned, you’d always be affected: there simply existed no majestic safe haven one could run to anymore.
He remembered sights of abandoned nations on the screen. It was always from the view of a drone’s camera eye, like a vulture over a dying beast. He watched as the skies blackened with smoke turned orange in the evening.
He remembered when events like these were only statistics. Some incomprehensible number rendered useless to the limitations of his mind.
It had always been just an incomprehensible statistic. Until the day came when he found himself living the statistic.
He remembered the screams and the sirens blaring and whistles shrieking.
Randall’s eyes shot open to the sound of his alarm clock’s shrill tone repeating over and over. His hand snapped over to slap the snooze button — he hadn’t meant to, he hated sleeping in, but it was the fastest way to turn it off.
He slowly pulled himself upright, legs hanging over the bed. His neck ached, and his heart was racing.
His eyes wandered around his lonely little home, across the pictures that adorned his walls, and finally out the window into his little world.
He stood up, placing his shaking hands on the window frame and gazed upon the land below. Just past the edge of the slums lay the patchy green and yellow suburbs, each house dark and empty, even in the early morning light. The occasional bird fluttered past. Just as frequently, the occasional drone.
He didn’t have nightmares very often, but his nightmares never changed.
He ought to be thankful, he reminded himself. Even if the nightmare doesn’t go away upon waking up, at least when you’re awake, there’s some chance of fighting it.
He hastily made his way to his cabinet to get dressed. Today was another big day full of appointments.
The I.E.B. Chronicles Chapter List: by Trevor Farrell
643 Views · View 24 Upvoters",Trevor Farrell,When will AI and automation take over 100% of all jobs?,Artificial Intelligence
5,"Using Neural Networks to accelerate circuit simulations.
For those who aren’t familiar, SPICE circuit simulation is basically a physics based method of modeling circuits, yielding (with good models), extremely accurate results, but at the cost of being painfully slow.
Enter RNNs. By using RNNs we can approximate these simulations and get a massive speedup. As an added bonus, the foundry’s proprietary transistor technology, which would otherwise be more vulnerable using conventional models, is stored in a “black box” RNN.
Of course, this comes with the downside of not being 100% accurate and you do have to stay within strict bounds to get anything decent and you also eventually have to run the “real” SPICE simulation to make sure what you’re taping out is actually correct and not a neural network’s hallucination. However, it’s still very useful for the designer to explore the design space without waiting forever for SPICE to finish.
1.4k Views · View 13 Upvoters · View Sharers · Answer requested by
Stewart Alsop",Tapa Ghosh,What is the most innovative use of a neural network that you have seen in 2018?,Artificial Intelligence
6,"Like this:
Please don’t tell my boss this. I still want to get paid and keep living.
TL;DR at the end. To start, let’s understand what those words mean.
When you ask “how do I code in […] artificial intelligence,” you’re talking about a really broad field. However, you don’t necessarily code in AI; rather, you code using a programming language (e.g., Python, C/C++, Java, R, JS). What you produce as a result of that can be considered AI (or “intelligence demonstrated by machines”).
With that out of the way, if you want to learn more about artificial intelligence (specifically machine learning), one great resource is Andrew Ng’s courses[1] (esp. the one on Coursera[2]). There’s also Artificial Intelligence: A Modern Approach by Peter Norvig and Stuart J. Russell[3] that explores more about AI in general—its ideas, concepts, and history.
If you want to try out machine learning, please consider use python, since it’ll be easier for you to learn the AI, ML, and/or DL concepts without having to struggle with the programming language itself. There are so many tutorials[4], online courses[5], books[6], and blogs[7] on how to start with machine learning. Even scikit-learn (ML library)[8] has tutorials[9] nowadays. You can even start learning more about machine learning by using some of the already implemented methods in scikit-learn; they even have examples!
Within machine learning, there’s supervised, unsupervised, and semi-supervised. Deep learning is a subset of machine learning that focuses on improving on improving learning…Okay. That may be a terrible explanation, but a lot of deep learning has to do with neural networks—artificial neural networks, recurrent neural networks, etc. Have you ever wanted to use a convolutional neural network to identify cats and dogs in photos?[10] No? Just me? OK. What about building a deep convolutional MNIST classifier[11], so you can maybe read your doctor’s writing?
At some point, you might want to know some more statistics, probability theory, linear algebra (esp. matrices), discrete math, etc., but you can study those things bit by bit as you encounter them. For statistics, you can try All of Statistics: A Concise Course in Statistical Inference by Larry Wasserman[12]. Other books include Machine Learning: A Probabilistic Perspective [13]by Kevin Murphy and Pattern Recognition and Machine Learning[14] by Christopher Bishop. At the end of the day, make sure you know the difference between baes and Bayes'[15].
One last thing, I wouldn’t say, “I code in machine learning.” It’s like saying “I compute in Geometry.” People will understand you, and they’ll look at you weird, esp. if they’re judgemental. I’ll let it slide this time since at one point I had thought neural networks were a bunch of if statements…and data science was just glorified stats. JK. About the first one.
Good Luck!
TL;DR
Roadmap for Machine Learning
Part 1: Why Machine Learning Matters. The big picture of artificial intelligence and machine learning — past, present, and future.
Part 2.1: Supervised Learning. Learning with an answer key. Introducing linear regression, loss functions, overfitting, and gradient descent.
Part 2.2: Supervised Learning II. Two methods of classification: logistic regression and SVMs.
Part 2.3: Supervised Learning III. Non-parametric learners: k-nearest neighbors, decision trees, random forests. Introducing cross-validation, hyperparameter tuning, and ensemble models.
Part 3: Unsupervised Learning. Clustering: k-means, hierarchical. Dimensionality reduction: principal components analysis (PCA), singular value decomposition (SVD).
Part 4: Neural Networks & Deep Learning. Why, where, and how deep learning works. Drawing inspiration from the brain. Convolutional neural networks (CNNs), recurrent neural networks (RNNs). Real-world applications.
Part 5: Reinforcement Learning. Exploration and exploitation. Markov decision processes. Q-learning, policy learning, and deep reinforcement learning. The value learning problem.
The above is from the Medium article Machine Learning for Humans by Vishal Maini. Just follow the roadmap. You’ll code in AI, ML, and DL in no time.
Disclaimer: results may vary.
Footnotes
[1] Courses - Andrew Ng
[2] Machine Learning | Coursera
[3] Artificial Intelligence: A Modern Approach
[4] ujjwalkarn/Machine-Learning-Tutorials
[5] Every single Machine Learning course on the internet, ranked by your reviews
[6] josephmisiti/awesome-machine-learning
[7] A Beginner’s Guide to AI/ML  – Machine Learning for Humans – Medium
[8] scikit-learn: machine learning in Python
[9] scikit-learn Tutorials
[10] Understanding deep Convolutional Neural Networks with a practical use-case in Tensorflow and Keras
[11] MNIST For ML Beginners  |  TensorFlow
[12] All of Statistics - A Concise Course in Statistical Inference | Larry Wasserman | Springer
[13] Machine learning textbook
[14] Pattern Recognition and Machine Learning | Christopher Bishop | Springer
[15] Bayesian probability - Wikipedia
50.7k Views · View 570 Upvoters · View Sharers",Peter Yuan,"How can I code in machine learning, artificial intelligence, and deep learning?",Artificial Intelligence
7,"Originally Answered: Is Deep Learning overhyped?
In many respects, it is. For sure, the recent successes of deep learning have been amazing: we went from being really terrible at supervised learning on perceptual problems (image classification, speech recognition) to being really good at it. Deep learning has been transformative for many subfields of machine learning. But here's the thing: lots of people, most of them not directly involved with deep learning research, tend to extrapolate too much from these recent successes. For instance, when we started achieving below 4% top-5 error on the ImageNet classification task, people started claiming that we had ""solved"" computer vision. We most certainly haven't solved computer vision at this point; it's still a tremendous challenge to generate accurate, precise descriptions of the contents of a picture or a video, or to get meaningful answers to basic visual queries (e.g. ""get me a close-up of the handbag of the second lady from the left""), things that humans take for granted. Our successes, which while significant are still very limited in scope, have fueled a narrative about AI being almost solved, a narrative according to which machines can now ""understand"" images or language. The reality is that we are very, very far away from that.
In the pitches of startups that are attempting to cash in on deep learning, I see a lot of grossly unrealistic expectations. Some of them are just naively over-optimistic, but some others are essentially living in a fictional universe —I've seen at least 3 different startups state that they would solve ""general artificial intelligence"" in the next few years. Best of luck to them. Most of these companies have no issue getting generously funded, but quite a few of them will find it very difficult to get a decent exit. A lot of disappointment will follow, especially among VCs and corporate decision makers, and unless this is counter-balanced by a larger wave of successful value-producing applications of deep learning, then we might witness a new AI winter in the future.
Overall: deep learning has made us really good at turning large datasets of perceptual inputs (images, sounds, videos) and simple human-annotated targets (e.g. the list of objects present in a picture) into models that can automatically map the inputs to the targets. That's great, and it has a ton a transformative practical applications. But it's still the only thing we can do really well. Let's not mistake this fairly narrow success in supervised learning for having ""solved"" machine perception, or machine intelligence in general. The things about intelligence that we don't understand still massively outnumber the things that we do understand, and while we are standing one step closer to general AI than we did ten years ago, it's only by a small increment.
197.8k Views · View 2k Upvoters · View Sharers · Answer requested by
Paul Balança,
Deepak Venkatesh, and 101 more",François Chollet,Is deep learning overhyped?,Artificial Intelligence
8,"The “classical” forms of deep learning include various combinations of feed-forward modules (often convolutional nets) and recurrent nets (sometimes with memory units, like LSTM or MemNN).
These models are limited in their ability to “reason”, i.e. to carry out long chains of inferences, or optimization procedure to arrive at an answer. The number of steps in a computation is limited by the number of layers in feed-forward nets, and by the length of time a recurrent net will remember things.
To enable deep learning systems to reason, we need to modify them so that they don’t produce a single output (say the interpretation of an image, the translation of a sentence, etc), but can produce a whole set of alternative outputs (e.g the various ways a sentence can be translated). This is what energy-based models are designed to do: give you a score for each possible configuration of the variables to be inferred. A particular instance of energy-based models is factor graphs (non-probabilistic graphical models). Combining learning systems with factor graphs is known as “structured prediction” in machine learning. There have been many proposals to combine neural nets and structured prediction in the past, going back to the early 1990s. In fact, the check reading system my colleague and I built at Bell Labs in the early 1990s used a form of structured prediction on top of convolutional nets that we called “Graph Transformer Networks”. There has been a number of recent works on sticking graphical models on top of ConvNets and training the whole thing end to end (e.g. for human body pose estimation and such).
For a review/tutorial on energy-based models and structured prediction on top of neural nets (or other models) see this paper: https://scholar.google.com/citat...
Deep learning is certainly limited in its current form, because almost all the successful applications of it use supervised learning with human-annotated data. We need to find ways to train large neural nets from “raw” non-annotated data so they capture the regularities of real world. As I said in a previous answer, my money is on adversarial training.
103.2k Views · View 874 Upvoters · View Sharers · Answer requested by 1135 people",Yann LeCun,What are the limits of deep learning?,Artificial Intelligence
9,"There are 6 types of positions at FAIR:
Research Scientist: you need a PhD, a couple years of experience in research (e.g. as a postdoc) and a good publication record. It’s a pretty high bar.
Research Engineer: you need a Master with some exposure to ML/AI in your previous studies or job. Most of these positions are relatively junior, but there are a few senior. About 25 to 30% of people at FAIR are research engineers.
Postdoc: it’s a 1 or 2 year limited-term research position, generally directly after your PhD.
PhD student: in our Paris lab, we can take a small number of PhD students under what’s called a CIFRE status. This is a special thing in France that allows PhD students to spend most of their time in an industry research lab, co-advised by a researcher in the company and a professor in a university.
Intern: we take summer interns, and sometimes interns during the academic year. Almost all of them are in PhD programs. In continental Europe where people do undergrad + 2 year Master + 3 year PhD, we take some interns between their Master and PhD.
Look here for a list of FAIR member and their backgrounds: Researchers
222.8k Views · View 1.2k Upvoters · View Sharers · Answer requested by
Gabriel Cypriano,
Jayant Raj, and 172 more",Yann LeCun,Is getting a masters or a PhD necessary to get into top AI/ML research groups like FAIR or DeepMind?,Artificial Intelligence
10,"0:00
-0:02
58k Views · View 3.7k Upvoters · View Sharers · Answer requested by
JC Miller",Emmaleigh Reynolds,What do we do when AI robots demand equal rights?,Artificial Intelligence
11,"One danger is you will cost your company a lawsuit and bad press when you are realize you are accelerating racist behavior. AI can learn human bias and if you are blindly pushing resumes through NLP libraries that is exactly what will happen.
Bad press:
The company BEAUTY.AI (incompetent) made international news when they shipped AI that judged beauty only to find out it only liked white people:
A beauty contest was judged by AI and the robots didn't like dark skin
Then FaceApp lightening people’s skin to make them more attractive:
Selfie app accused of racism for whitening users’ skin to make them 'hot'
This is embarrassing. It turns out that the underlying beauty data from the humans is racist. Here are some face averages I did of average attraction, see the skin tone bias?
It turns out this skin tone bias on attraction even exists in countries that are all black in Africa. Cause? You could argue colonialism or Hollywood influence maybe? The reality is humans all of unconscious bias and this is a nasty one.
This was a piece I did going deeper on AI and human bias:
https://www.linkedin.com/pulse/f...
I’ve also presented to the EEOC in DC on racist AI and tools for adverse impact mitigation. I am convinced that the only way to fix human racism is with AI, so there is a silver lining to this.
Conclusion:
You need to understand what you are doing. Blindly taking algorithms and pushing data in and out can expose you and your employer to legal liabilities you may not have anticipated.
38.1k Views · View 536 Upvoters · View Sharers",Ben Taylor,What are the dangers of using machine learning libraries without any understanding?,Artificial Intelligence
12,"I've read a lot of Andrew's comments, so I'll mention a few things that I've come across.
He clearly has very high natural intelligence and a huge amount of energy.
He's been at it for a very long time. He was heavily involved in ML research as an undergrad, and may well have been working on learning AI/statistics/etc before college.
Because he's been consistently exposed to the best (CMU, MIT, Berkeley, Stanford), he's been exposed to what it takes to do great work.
He reads papers for two hours a day, which he says leads him to good ideas. I've noticed that among ML/CV/NLP researchers, for a lot of them, you see them with an IDE open all day long, and they probably don't read enough. Those who read more tend to have better ideas, and access to a wider range of tools.
While his explanations are extremely lucid, this was apparently not always the case. During his first year as a Professor, he got terrible teaching reviews. He worked at it until he became excellent.
Check out my blog on statistics and machine learning here.
206.9k Views · View 976 Upvoters",Alexander Moreno,How did Andrew Ng become so good at Machine Learning?,Artificial Intelligence
13,"Here are some of the negative aspects of working in ML right now:
Competition: Getting into a graduate program in AI has become insanely competitive. Getting a satisfactory job related to machine learning after an MS might be difficult, because most MS students in CS have done some amount of ML and want to get a position related to ML.
Review quality: NIPS 2018 had close to 5000 submissions! Other conferences are not particularly different. This makes the conference committee’s job hard, and sometimes the review quality gets compromised (because you can’t find enough established researchers to review so many papers).
Volume of papers: Regardless of whether a paper gets accepted or not, it goes on arXiv. With hundreds of papers uploaded on arXiv every week related to AI, it is practically impossible for people working in AI to be up-to-date with most new research.
Getting lost in the noise: With all these papers, the few very high-quality papers might not stand out, as they would have had there not been so many papers. Similarly, on the startup front, everyone seems to be using AI, so that companies that are actually doing something non-trivial with AI might not get enough attention.
66k Views · View 979 Upvoters",Prasoon Goyal,What are the dark sides of a career in AI/machine learning?,Artificial Intelligence
14,"I doubt it.
Given my background — being affiliated with Google and having use TensorFlow for a long time — you may find my answer biased. However, I will try to be objective and say that PyTorch is not the overall best AI framework for developing deep learning neural networks.
Point 1. PyTorch is not easier to learn; it’s just more similar to what you expect.
Many PyTorch users like PyTorch for its simplicity to use, especially when they compare PyTorch to other frameworks that involve building static graphs such as TensorFlow or Theano. However, I argue that this is not because PyTorch is better designed, but more because PyTorch provides a platform that is closer to the imperative programming paradigm, which is the paradigm that most people have used since Programming 101.
TensorFlow requires some adjustments, e.g. debug by printing is no longer straightforward. That said, when one gets familiar enough with TensorFlow, all these gaps will be bridged, and debugging is more effortless.
Point 2. There are things that you cannot do with PyTorch.
I challenge anyone who thinks PyTorch is the overall best framework for deep learning to replicate the experimental results in this paper using PyTorch. The paper suggests that one can use reinforcement learning to assign different operations on a computational graph of a neural network to some devices available, then run the placement and measure the running time, which is used as the reward to train the reinforcement learning algorithm. Implementing the idea involves the following steps:
Extract the computational graph of a deep learning model into a simple object to manipulate. I don’t know if PyTorch allows this level of micro-manipulation to its dynamically constructed graphs. In TensorFlow, while the graphs are static, the pseudo code to manually place all computational operations on a device is somewhat as follows:
for op in graph.list_of_ops:
  op.device = ""/gpu:1""
Run the placed graph and measure the running time. This step may be framework agnostic, as it just involves calling `time.time()`.
Use the reward to train the reinforcement learning scheduler. This step may be easier to do using PyTorch than using TensorFlow or Theano.
Run step 1) above on 640 replicas, each of which has 4 GPUs and 1 CPU of the same type. All the rewards have to be collected and sent to the 641-st GPU, which houses the reinforcement learning agent. Gradients have to be collected and synchronized. Okay, before you jump on me saying that nobody has 640 computers, each with 4 GPUs (as a matter of fact, some do), let me get to my point: PyTorch is not designed to implement distributed learning systems at this scale.
Okay, okay, the example above is my own paper, so it sounds narcissistic. How about AlphaGo? Everyone likes AlphaGo, right? AlphaGo is deployed on 1,202 CPUs and 176 GPUs. As of May 6th 2018, I don’t see PyTorch operates at this scale.
Point 3. PyTorch has its weirdos, too.
It took me a while to really fathom what the @ sign does in PyTorch, e.g. in torch.bmm. When I first saw this sign in a github repository, I was perplexed. Even now, I don’t have a concrete explanation of what this @ sign means. My best guess is, of course, it refers to a multiplication. However, multiplication in which context? How is the operation broadcasted? I failed to find any official documentations that confirm the usage of this operation.
I believe there are other weirdos in PyTorch. I haven’t used the platform extensively enough to have all of them at the top of my mind. That said, I think it’s unfair to judge TensorFlow for sess.run(), by MonitoredSession, or simply by the fact that you cannot print anything to debug.
For what it’s worth, there is no overall best AI framework for developing neural networks, just like there is no overall best programming language.
The final message that I want to deliver in this answer is that one should try to be observant and to be knowledgeable, so as not to reject a tool just because the tool is hard to learn and to use. Depending on one’s usage, one’s experience, and the resources that are available, one will find the best suitable framework one’s work.
35.6k Views · View 193 Upvoters",Hieu Pham,Is PyTorch the overall best AI framework for developing deep learning neural networks?,Artificial Intelligence
15,"Things always come and go. DL is super hot right now, has been hot since mid 2012, but it’s not necessarily the case that it will still be the center of ML in, say, 2022 or 2032. About ten years ago, “Bayesian” was the previous buzz word which many claimed to be the “future of ML”; people like Michael Jordan et al were drawing “plate diagrams” all over the world, injecting their over-complicated “priors” into people’s minds (well, I never believed those stuff, and I rejected every single applicant if s/he said s/he wanted to work on “Bayesian” just because it sounded “cool”. I just don’t like people who like trendy stuff; I prefer people who believe in deeper, non-trendy, stuff). These hypes all faded away pretty quickly.
DL, however, is nothing like “Bayesian”. It is much more fundamental (though not necessarily “deeper”, see below), and has brought a much more serious change to ML and AI in general. So obviously DL has stayed and will stay much longer than the short-lived “Bayesian” hype.
But even neural net itself was kinda a taboo word in the 90s and early 2000s when the center stage of ML was occupied by SVMs and graphical models. Students were not allowed to work on NN in many universities around the world. As a side story, when I was interviewing for a faculty job at CMU just before DL really took off, we were discussing over the dinner about the new buzz word “deep learning”, and one CMU professor (let’s call him prof. A) cried, “c’mon it’s just neural nets!!”, and other professors, including myself, all cheerfully agreed (well, I still believe “it’s just neural nets” today, and like many colleagues, I am ethically against the unethical rebranding of NN as “DL” since there is nothing “deep” there. As known to all, “DL” sounds way too offensive to anybody who works on the more theoretical or structured areas in AI, including NLP and structured prediction. The fact I use the term “DL” doesn’t suggest my approval of it. I personally prefer to call DL “shallow models” instead). Today, however, everybody on that dinner table has had (at least quite a few) DL publications; interestingly, prof. A himself, being the most productive among us, wrote way more DL papers than everybody else combined.
During the long “NN winter”, very few people kept working on it, not even Hinton himself, but Schmidhuber and Lecun, among a few others, persisted. And for this reason I think Schmidhuber is probably the most underrated ML researcher. He deserves the same status as Hinton, Bengio, and LeCun. The history is not what Bengio et al claimed, “we are the first three labs that took the deep learning approach, soon followed by Andrew Ng at Stanford and others…” The real history is that Schmidhuber kept working on what is now called DL and made one of the two most important contributions in it (the other being LeCun’s CNN) 15 years before DL finally took off.
Interestingly, the same story happened to statistical machine translation. After the groundbreaking IBM 1990 paper first came out, people were excited about it, but that excitement soon faded away, after Renaissance bought the whole group and IBM stopped MT research. During the mid-late 1990s a very small number of people persisted, including most notably Dekai Wu of HKUST and Hermann Ney of Aachen, followed by Kevin Knight of USC/ISI. When statistical MT came back and took off around 2003, these three people soon became some of the most cited people in NLP, when people dig up their old work done in the “SMT winter”, just like people are digging up Schmidhuber’s old work done in the “NN winter” like crazy.
These pioneers deserve the recognition because they were way ahead of time; they kept working on what they believed in during a time when the majority follows the trend.
Now that DL is in fashion and other parts of ML (such as structured prediction) are slightly out of fashion, it is, in my opinion, exactly the best time to believe in yourself, keep working on what you believe in, and if you are lucky, become the next Schmidhuber or the next Dekai Wu.
One side note though: ML has been different now because of DL. So even if you don’t work in DL or don’t believe in it, you should consider updating your arsenal with DL tools, as the other answers also suggested.
Another personal footnote: when I had my first paper with one of the pioneers above, Kevin Knight (and with Hao Zhang and Dan Gildea), the reviews were generally positive except for one; and I said to Kevin, “that reviewer probably just didn’t understand it because our work is ahead of time”, to which Kevin responded, “No, we were not. It was Dekai Wu who was 10 years ahead of time!” I immediately realized that he was absolutely right. [for those of you not in NLP, Kevin was referring to the 10 year gap between the original ITG (Wu, 1995; 1997) and its award-winning and groundbreaking modernization “Hiero” (Chiang, 2005; 2007) which changed the whole field. Even today, many people in NLP, including myself, strongly believe that Chiang (2005) is, and will probably still be for a long time, the single most beautifully written conference paper in the entire ACL history, and that Chiang (2007) is one of the most elegantly written journal papers in CL. But it was Wu (IJCAI 1995; CL 1997) that was more original, although it was obscure during the SMT Winter, being cited ~3 times per year (including, ironically, many self-citations) in that 10-year gap. In other words, ITG was more or less obscure back then, but suddenly came to the center stage in 2005 out of nowhere thanks to David’s groundbreaking work, a little like LSTM suddenly became the focus on ML out of nowhere 10+ years after its publication.]
Historical footnote: actually there have been many other “winters” in the histories of both NN and MT. The specific “NN winter (c. 1994–2005)” and “SMT winter (c. 1993–2002)” referred to in my answer above were the most recent ones in those two areas, respectively. The first NN winter dates back to 1969 due to (the misinterpretation of) Minsky/Papert, and the first MT winter as early as 1966 due to the ALPAC report. As I said in the beginning, things come and go. There will certainly be another NN or AI winter down the road, but there is nothing to worry about, since they all come and go.
195.9k Views · View 945 Upvoters · View Sharers",Liang Huang,"Should I quit machine learning? I used to find it exciting before it was cool, but now I find it unexciting and boring. Since deep learning is the future, should anyone with no interest in deep learning quit?",Artificial Intelligence
16,"In my 4 years of working in deep learning at NVIDIA, I worked as a solution architect to research deep learning techniques, and present possible solutions to customers to solve their problems and help implement them.
Since then, AI, or Artificial Intelligence, has become a very commonly applied term today, and it is often used ambiguously, or even incorrectly to describe deep learning and machine learning. Let me write down some extremely simplistic definitions of what we do have today, and then go on to explain what they are in more detail, where they fall short, and some steps towards creating more fully capable 'AI' with new architectures.
Machine Learning - Fitting functions to data, and using the functions to group it or predict things about future data. (Sorry, greatly oversimplified)
Deep Learning - Fitting functions to data as above, where those functions are layers of nodes that are connected (densely or otherwise) to the nodes before and after them, and the parameters being fitted are the weights of those connections.
Deep Learning is what what usually gets called AI today, but is really just very elaborate pattern recognition and statistical modelling. The most common techniques / algorithms are Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Reinforcement Learning (RL).
Convolutional Neural Networks (CNNs) have a hierarchical structure (which is usually 2D for images), where an image is sampled by (trained) convolution filters into a lower resolution map that represents the value of the convolution operation at each point. In images it goes from high-res pixels, to fine features (edges, circles,….) to coarse features (noses, eyes, lips, … on faces), then to the fully connected layers that can identify what is in the image. The cool part of CNNs is that the convolutional filters are randomly initialized, then when you train the network, you are actually training the convolution filters. For decades, computer vision researchers had hand-crafted filters like this, but could never get results as accurate as CNNs can get. Additionally, the output of a CNN can be an 2D map instead of a single value, giving us a image segmentation. CNNs can also be used on many other types of 1D, 2D and even 3D data.
Recurrent Neural Networks (RNNs) work well for sequential or time series data. Basically each 'neural' node in an RNN is kind of a memory gate, often an LSTM or Long Short Term Memory cell. When these are linked up in layers of a neural net, these cells/nodes also have recurrent connections looping back into themselves and so tend to hold onto information that passes through them, retaining a memory and allowing processing not only of current information, but past information in the network as well. As such, RNNs are good for time sequential operations like language processing or translation, as well as signal processing, Text To Speech, Speech To Text,…and so on
Reinforcement Learning is a third main DL method, where you train a learning agent to solve a complex problem by simply taking the best actions given a state, with the probability of taking each action at each state defined by a policy. An example is running a maze, where the position of each cell is the ‘state’, the 4 possible directions to move are the actions, and the probability of moving each direction, at each cell (state) forms the policy.
By repeatedly running through the states and possible actions and rewarding the sequence of actions that gave a good result (by increasing the probabilities of those actions in the policy), and penalizing the actions that gave a negative result (by decreasing the probabilities of those actions in the policy). In time you arrive at an optimal policy, which has the highest probability of a successful outcome. Usually while training, you discount the penalties/rewards for actions further back in time.
In our maze example, this means allowing an agent to go through the maze, choosing a direction to move from each cell by using the probabilities in the policy, and when it reaches a dead-end, penalizing the series of choices that got it there by reducing the probability of moving that direction from each cell again. If the agent finds the exit, we go back and reward the choices that got it there by increasing probabilities of moving that direction from each cell. In time the agent learns the fastest way through the maze to the exit, or the optimal policy. Variations of Reinforcement learning are at the core of the AlphaGo AI and the Atari Video Game playing AI.
One last note goes to GANs, or Generative Adversarial Networks, which is more of a technique than an architecture. Currently it is used with CNNs to make image discriminators and generators. The discriminator is a CNN that is trained to recognize images. The generator is an inverse network that take a random seed and uses it to generate images. The discriminator evaluates the output of the generator and sends signals to the generator on how to improve, and the generator in turn sends signals to the discriminator to improve its accuracy as well, going back and forth in a zero-sum game till they both converge to best quality. This is a way of providing self-reinforcing feedback to a neural system, which we will revisit later.
Of course, there are rich variations and combination of all these methods, as well as many others, and combined, these are the bread and butter of Deep Learning, which is what we call AI today, but perhaps prematurely, as these methods do not have any cognition, intelligence, or intuition, and are more brute-force statistical analysis / pattern recognition and often require large amounts of (labelled) data to train to a given standard.
Although they can work very well for model problems and benchmarks, these techniques sometimes do not scale or work as well once you try to apply them outside those specific problems they were designed for. For real-world problems, they sometimes do not perform as well, even if you can scale-up and redesign the network topology and tune them. Sometimes we just do not have enough data to train them sufficiently to make them robust and accurate in deployment. Or perhaps the real-life problem just can't be quantified well enough, for example the Imagenet image classification competition has 1000 object classifications, but in a real-life application, there are probably millions of object classifications and sub classifications. To get the DL systems to do new things, or recognized new data, they have to be re-trained constantly and re-deployed, and cannot learn in-situ, on the fly in the field.
As well, many applications require combining multiple DL techniques together and finding ways to fuse them. A simple example is video tagging - you pass the video frames through a CNN, and at the top have an RNN to capture the temporal behavior of the features in those videos with time. As an example, I helped a researcher/entrepreneur use this technique to recognize facial expressions of a quadriplegic to issue commands to their wheelchair and robotic prosthesis, with a different facial expression/gesture paired with each command. It will work, but as you scale it up, it may be time consuming and tricky to develop and train, because you now have to tune two different types of DL networks that are intertwined, and it is sometimes hard to know what effect these tweaks are having.
Now imagine you had multiple of these CNN/RNN networks feeding input, a deep reinforcement learning engine making decisions on this input state, then driving generative networks creating output. These are a lot of specific DL techniques hacked together to accomplish a set of tasks. Can you say debugging and tuning hell? Will it even work? I don’t know, but if it does it will cost a lot and take a long time to get working, and you’d have to be very creative to isolate and test the systems and progressively integrate them, and it is uncertain whether it could train well enough in combination to even perform in real-world conditions.
My personal opinion is that our current DL techniques each represent a reduced sub-set of how a brain’s networks and our nervous system work, each like a different 2D projection or shadow, or a discretized subset of the real thing, with functionality that sorta, kinda works for certain cases, and we call it 'neural', but it really is not. They are each specialized to specific tasks.
In fact, what most people practicing DL or 'AI' today don't realize is that today's 'neural networks' and 'neurons' in deep learning are just the simplest subset of a much larger and richer family of synthetic neurons, neural networks and methods. Most of the layered neural networks and CNNs we use in DL today fall into a smaller family called Feed-Forward Neural Networks, which simply sum the weighted inputs at each node, apply a simple transfer function, and pass the result to the next layer. This is not an accurate model of how the brain works by any means, and even RNNs and reinforcement learning are not giving us true artificial intelligence, but just fitting the parameters of very large and complex functions to large amounts of data, and using statistics to find patterns and make decisions.
The methods top and left, especially the Spiking Neural Networks, give a more accurate model of how real neurons operate, with simple, compute-efficient models like 'Integrate and Fire', and 'Izhikevich' to more complex models like 'Hodgkin-Huxley' that come close to modelling a biological neuron's behavior, and modelling how networks of them interact and work in the brain, opening up much richer neural computational models.
In real neurons, time-domain signal pulses travel along the dendrites, and then arrive at the neuronal body independently, and are integrated in time and space inside it (some excite, some inhibit). When the neuronal body is triggered, it produces a time-dependent set of pulses down its axon, that split up as it branches and take time to travel out to the synapses, which themselves exhibit a non-linear, delayed, time-dependent integration as the chemical neurotransmitter signal passes across the synapse to eventually trigger a signal in the post-synaptic dendrite. There is a strengthening of the synapse, or learning, in the process, if the neurons on both sides of it fire together within a certain interval, called Hebbian learning. We may never be able to completely replicate all the electrochemical processes of a real biological neuron in hardware or software, but we can search for models that are sophisticated enough to represent much of the useful behavior needed in our spiking artificial neural networks.
This will bring us closer to more human-like AI, as real brains get much of their computing, sensory processing and body-controlling capability from the fact that signals ‘travel’ through neurons, axons, synapses, and dendrites, and thus travel through the brain structures, in complex, time-dependent circuits that can even have feedback loops to make circuits like timers or oscillators, or neural circuits that activate in a repeatable, cascading pattern to send specific time-dependent patterns of controlling signals to groups of muscles/actuators. These networks also learn by directly strengthening connections between neurons that repeatedly fire, called Hebbian learning. For doing more complex AI and decision making, they are much more powerful than the CNNs, static RNN’s and even deep reinforcement learning that we use in our above examples.
But there is one huge drawback - there are no current methods for fitting these kinds of networks to data to ‘train’ them. There is no back-propagation, nor gradient descent operations that tune the synaptic weights between neurons. The synapses just strengthen or weaken, and so the spiking neural network learns as it goes about its business of operating, using Hebbian learning as it goes, which may or may not work in practice to train our synthetic networks, as they have to be structured correctly in the first place for this to converge to a useful solution. This is an area of ongoing research, and a breakthrough in this area could be very significant. Below are my ideas, from the ORBAI provisional patent(US 62687179, filed Jun 19, 2018):
I will first describe an approximation to how we best understand that the visual cortex works, with not only images from our retinas being processed into more high-level and abstract patterns and eventually ‘thoughts’ as they move deeper through the different, higher levels of the visuals cortex in the brain (similar to the classic CNN model), but also with thoughts cascading the other direction through the visual cortex, becoming features, and eventually images on the lowest levels of the cortex, where they resemble the images on our retinas. Just pause for a minute, close your eyes, and picture a ‘fire truck’… see it works, you can visualize, and perhaps even draw a fire truck, and in doing so, you just used your visual cortex in reverse, and CNNs cannot do that. But because our visual cortex works like this, we are always visualizing what we expect to see, and constantly comparing that with what we are actually seeing, at all levels of the visual cortex. Sensing is a dynamic, interactive process, not a static feed-forward one.
This describes a method for training a artificial neural (either spiking, or feed-forward) network where there are two networks that are intertwined and complementary to one another, with one transmitting signals in one direction, say from the sensory input, up through a hierarchical neural structure to more abstract levels to eventually classify the signals. There is also a complementary network interleaved with it that has signals that flow in the opposite direction, say from abstract to concrete, and from classification to sensory stimulus. The signals or connection strength in these two networks can be compared at the different levels of the network and the differences used as a ‘training’ signal to strengthen network connections where the differences are smaller and correlation tighter, and to weaken network connections where the differences are larger and not as tightly correlated. The signals can be repeatedly bounced back and forth off the highest and lowest levels to set up a training loop, almost like dreaming?
If this works well for synthetic neural networks, the result could be profound, and we can now ‘train’ these neural networks, while they operate, in-situ, in real-time (Think 'Chappie'). This is far more dynamic, useful, and powerful than back-propagation and gradient descent during dedicated training for CNNs, and this wraps the functionality of (self-training) CNNs, GANs, and even Autoencoders into a single, more elegant architecture (which is expected if we are moving from special purpose 'hacks' to a more functional, robust, and brain-like network and neurons). With a bit of ‘retrofitting’, perhaps this technique could be used on a standard feed-forward CNN with an inverse, feedback CNN interleaved to train it as well.
Going back to GANs, they are close cousins with these feedforward / feedback interleaved networks, because the fwd/back networks are inverses of each other, and each train one another. The difference is that GANs are loosely coupled with specific interface points, but the feedforward / feedback networks can be very densely connected and as tightly coupled as you wish. This is huge, because one of the largest difficulties with GANs is determining the feedback method and signals to send between the generator / discriminator, and for other than simple data types like images it can get complicated. The feedback / feedforward method can work for any arbitrary system - vision, hearing, speech, motor control,.... because the training and communication method is intrinsic to the system, adapted to the network architecture. Yes, just like human neural systems, these are multipurpose, elegant, and very powerful.
Another problem with spiking neural nets is how do you connect the neurons in the first place? Sure, we can train the networks and strengthen/weaken synapses once we get rolling, but how do you even figure out how to construct them in the first place. I'm not going to go into enormous detail, but start with small hand-wired networks, use genetic algorithms to explore the design space, training with the above technique towards simple performance metrics, then assign a gene to each subnet that works well and start replicating them into bigger networks, again use genetic algorithms to shuffle the genes (and the subnets), train against more complex performance metrics, and keep iterating, hierarchically building larger networks at each step and assigning 'genes' to each construct at each level. Since the entire human brain develops from the information encoded in 8000 genes to a 100 billion neuron, 100 trillion synapse structure, it seems that this hierarchical encoding would be the only way to do this in a natural or synthetic neural system and we just have to figure out the details. In the provisional patent mentioned above, I call this collection of methods (and others) NeuroCAD.
One other drawback to implementing and scaling spiking neural network is that while they are capable of sparse computation (neurons and synapses only compute when a signal passes through them) and could be very low-power, most hardware we can run them on today like CPUs and GPUs compute constantly, updating the model for every neuron and synapse every time-slice (there may be workarounds). Many (large and small) companies are working on neuromorphic computing hardware that more closely matches the behavior of spiking neurons and neural networks in function and is able to compute sparsely, but it is difficult to provide enough flexibility in the neural model and to interconnect, scale, and train these networks at present, especially at network sizes large enough and organized properly to do useful work. A human brain equivalent would require over 100 billion neurons and 100 trillion synapses, which is far beyond the density of any 2D chip fabrication technology we have today. We will need new fabrication technologies, and probably 3D lattices of synthetic neurons, axons, dendrites and synapses to get there. Ouch, who wants to build the first fab?
I think if we can start solving these issues, and move towards more functional neuromorphic architectures that more fully represent how the brain, nervous system, and real neurons work and learn, we can start to consolidate some of the one-of, specific Deep Learning methods used today into these more powerful and flexible architectures that handle multiple modes of functionality with more elegant designs. As well, with these models, we will open up novel forms of neural computation and we will be able to apply them to tasks like computer vision, robot motor control, hearing, speech, and even cognition that is much more human-brain-like.
But will more sophisticated neural networks like this actually work in the end? Go look in the mirror and wave at yourself - yes it CAN work, you are that proof. Can we replicate it in an artificial system as capable as you? Now there is the trillion dollar question, isn’t it?
Brent Oster (www.orbai.ai)
If you are interested in how this could translate into future advanced Strong General Artificial Intelligence, see:
Brent Oster's answer to Is Strong AI really achievable?
20.5k Views · View 157 Upvoters · View Sharers",Brent Oster,"Why has deep learning become so famous? In my experience, it’s not applicable to the majority of real world problems.",Artificial Intelligence
17,"Don't do it. I've participated in Kaggle competitions for 2 years, and went from knowing very little to having won a competition and being ranked as high as 12th in Kaggle ranking. At the beginning I just couldn't get enough of Kaggle.
It lasted about a year. Then as you start doing better, you end up in a cycle: you want higher rankings, you do more competitions, your opportunity for learning becomes incrementally smaller, your time is spread very thin... and if you do very well, like competing for a win, it can become very stressful. The virtous cycle had slowly become a vicious cycle. I was burned out after a year and a half, and only kept participating in competitions for the sake of maintaining my ranking. Then I realized I had passed a point in my learning curve where I was becoming better ""at Kaggle"", instead of becoming better at ML. There is where I just decided to stop and move on to learning something new and different.
Long story short, be careful what you wish for.
351.5k Views · View 733 Upvoters",Giuliano Janson,I have caught a serious bug of learning Machine Learning. And I want to participate in kaggle competitions. I am thinking of leaving my current job for this process. What is your advice on this?,Artificial Intelligence
18,"This one tries to predict which Indian state you’re from, based on your name. Given that there are several character-level patterns in Indian names which may identify the person’s home state, I was surprised that this hasn’t been done before (at least publicly). I’m still working on it, but here’s a snippet:
size = len(names)
train_X = np.array(names[:size * 2/3])
train_y = np.array(indStates[:size * 2/3])
test_X = np.array(names[size * 2/3:])
test_y = np.array(indStates[size * 2/3:])
X = tf.placeholder(tf.float32, [None, max_sequence_length, num_input])
y = tf.placeholder(tf.float32, [None, num_classes])
weights = weight_variable([num_hidden, num_classes])
biases = bias_variable([num_classes])
rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_hidden)
out, states = tf.nn.dynamic_rnn(rnn_cell, X, dtype = tf.float32)
y_ = tf.matmul(outputs[:,-1,:], weights) + biases
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_, labels = y))
train_step = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)
Here’s one that isn’t mine.
This one is Google’s image captioning model. A snippet and a few examples of generated captions are below, but the whole codebase is available here.
But wait! In case you were too impressed, here are some epic fails by the same model (as shown in https://arxiv.org/pdf/1609.06647...):
Artificial Intelligence will probably end up being as stupid as the rest of us.
356.7k Views · View 5.6k Upvoters · View Sharers",Sriraman Madhavan,What does AI code look like?,Artificial Intelligence
19,"A classic program inside a robot, it is not an AI.
(Also a program made with thousands of “IF”, it can do rather complex stuff, but it is a pain to mantain.)
The AI in search engines are perfect and what they recommend is the best for their users.
But instead the most of the time, they just play safe with funny viral content so they can sell more advertisements.
AI will make coders obsolete.
Nope. What make coders not necessary is easy to use configuration tools (not AI at all), kind of like self service make cashiers not necessary.
And of course to make configuration tools you need coders. The worry is legit about cashiers as they don’t know how to make self checkout boots or vending machines.
Self driving AI will for sure be safer than human driving.
It really depends a lot how it is done. Silicon valley startups, which are to eager to sell unstable products with crowd sourced data or that can be bankrupt after a year, are maybe not the best companies to trust our life to.
Let’s put AI in everything! Now with 20% more AI!
Now everything is called AI, even if it doesn’t have effectively a neural network.
I am not even joking. The mind in the machine: smartphone chips with Artificial Intelligence | AndroidPIT
Will robots steal our job, if they can learn how to do the same things human can do?
Robots can already do much more difficult tasks that humans can’t do, because we literally programmed them to do that.
More coming if you like.
Thanks to Commitstrip, they make awesome webcomics.
56.1k Views · View 2k Upvoters · View Sharers",Valerio Cietto,What are the biggest misconceptions about AI?,Artificial Intelligence
20,"[1412.6572] Explaining and Harnessing Adversarial Examples
To you and me these are two pictures of a panda. While a neural network would agree with us on the left hand picture, it is absolutely certain that the right hand picture shows a gibbon. And we cannot explain why.
Since we don’t understand why it insists that the right picture shows a gibbon, we also don’t really understand why the neural network thinks the left picture shows a panda.
When we look at the neural network we can identify individual neurons which tend to activate when certain features are present, like two tilted black ovals in the middle of a big white circle. So it’s tempting to claim that it thinks the picture shows a panda because these features are present, but the same features are also present in the other picture, so this isn’t a correct description of how the neural network works, and we don’t really understand how it really works.
There are some things we do understand. The picture on the right is called an adversarial example and it was creating by perturbing the picture on the left by the noise term shown in the middle. And we understand how we can find such noise terms. We have also made progress in understanding why neural networks are susceptible to such images, which has a lot to do with the curse of dimensionality and the decision boundary between pictures of different objects. But unfortunately we don’t understand this well enough to actually fix it.
You might think, we could simply fix this by providing additional learning data, including the picture on the right. Unfortunately this doesn’t work. While this allows us to defend against specific examples, it’s always possible to create new examples. And all network architectures we have explored so far are susceptible to adversarial examples.
21.5k Views · View 548 Upvoters",Markus Schmaus,What does it mean that we don't really understand what happens in neural networks?,Artificial Intelligence
21,"Surveillance.
Predictive analysis.
4 Views · View 1 Upvoter · Answer requested by
Leonardo Zanobi",Syed Mustafa,What’s the field that A.I. is helping the most?,Artificial Intelligence
22,"There is tons of on-line material, tutorials and courses on ML, including Coursera lectures.
I’ll respond more specifically for deep learning. You can get a broad idea of deep what deep learning is about through tutorial lectures that are available from the Web. Most notably:
an overview paper in Nature by myself, Yoshua Bengio and Geoff Hinton with lots of pointers to the literature: https://scholar.google.com/citat...
The Deep Learning textbook by Goodfellow, Bengio and Courville: Deep Learning
A recent series of 8 lectures on deep learning that I gave at Collège de France in Paris. The lectures were taught in French and later dubbed in English:
French version: Accueil
English version: Home
the Coursera course on neural nets by Geoff Hinton (starting to be a bit dated).
the lectures from the 2012 IPAM Summer School on Deep Learning: Graduate Summer School: Deep Learning, Feature Learning (Schedule) - IPAM
my 2015 course on Deep Learning at NYU: deeplearning2015:schedule | CILVR Lab @ NYU (unfortunately, the videos of the lectures had to be taken down due to stupid legal reasons, but the slides are there). I’m teaching this course again in the Spring of 2017.
The 2015 deep learning summer school: Deep Learning Summer School, Montreal 2015
Various tutorials generally centered on using a particular software platform, like Torch, TensorFlow or Theano.
205.9k Views · View 3.2k Upvoters · View Sharers · Answer requested by 1450 people",Yann LeCun,What are your recommendations for self-studying machine learning?,Artificial Intelligence
23,"Target found out the pregnancy of a teenager before her parents did.
An angry father walks into a Target store in Minneapolis, demanding to talk to the manager:
“My daughter got this in the mail!” he said. “She’s still in high school, and you’re sending her coupons for baby clothes and cribs? Are you trying to encourage her to get pregnant?”
A few days later:
“I had a talk with my daughter,” he said. “It turns out there’s been some activities in my house I haven’t been completely aware of. She’s due in August. I owe you an apology.”
Source: How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did
Turns out, Target had a system which assigns each shopper a “pregnancy prediction” score based on the products they buy. The system could also estimate their due date to within a small window, so Target could send coupons timed to very specific stages of their pregnancy.
This happened in 2012 and it’s hardly state-of-the-art “AI”, but it just goes to show that anything creepy a machine learning model does, is just a product of how and with what data it is trained.
And that includes Facebook bots communicating strangely, neural networks generating creepy dog images, Microsoft chatbot’s genocidal tweets, etc.
At the risk of getting sued by the NRA, keep in mind: AI systems don’t creep people out. People creep people out.
303.5k Views · View 15.6k Upvoters · View Sharers",Sriraman Madhavan,What is the creepiest thing any AI has done so far?,Artificial Intelligence
24,"A major difference between US AI and China AI is that China AI is all about implementation.
In research, US has about 60% of the world’s top 1000 top researchers, and China less than 10%. The top US researchers are both academia and industry, while the top Chinese researchers are generally in the industry, while academia lags behind the US substantially. Chinese research papers have increased in quality rapidly over the years, but it will take a long time to catch up with the US. (For more details on this, see my book AI Superpowers)
Deep learning is the single greatest invention so far in the Era of Discovery, which was led by the U.S.. But since the deep learning breakthrough, we’ve already entered the Era of Implementation where what matters is execution, product quality, speed, and data. And that’s where China comes in.
China’s technological execution is built on incredible work ethic. Nearly abandoning of my wife in the delivery room is nothing compared to the entrepreneurs in China. As a venture capitalist in China, I once saw a startup claim that it offered great work-life balance because it was “996”. What’s 996? 9am to 9pm, 6 days a week. Most other startups in China are 997.
Chinese product quality has improved dramatically due to intense competition. Silicon Valley competition resembles the old wars where each side takes its turn to fire. In China, competition is like gladiators in the coliseum, fighting to the death with no holds barred. Fierce competition pushes entrepreneurs to improve the product at lightning speed, and to develop impregnable business models. As a result, Wechat and Weibo have evolved into arguably better products than products from Facebook and Twitter.
Chinese market rapidly embraces new products and new paradigms. Just within the last 3 years, mobile payments have emerged as the dominant transaction tool, replacing cash and credit cards. Total transaction in 2017 was $18.8 trillion, even larger than China’s GDP. How’s that possible? China’s mobile payments are built on the world’s best infrastructure: nearly zero-transaction-fee, micropayment-capable, and peer-to-peer. Over 700 million Chinese users can pay each other, whether for online, offline, loan, or gift, whether to your child, a farmer in a village, or even a beggar.
All of this is amplified by China’s enormous market size, which generates the treasure trove of data which is the critical rocket fuel for AI. China’s data edge is 3 times the US based on mobile users ratio, 10 times the US in food delivery, 50 times in mobile payment, and 300 times in shared bicycle rides. (The few paragraphs above come from my TED talk this year).
All this rich data is used to make Chinese companies’ AI work better. Today, China has the world’s most valuable companies in computer vision, drones, speech recognition, speech synthesis, and machine translation. The total valuation of Chinese computer vision companies is about $10B, and the total valuation of Chinese speech recognition companies is also about $10B.
Internet is an area where AI giants blossomed. The same is true for US and China. For US: Google, Facebook, Amazon, Microsoft. For China: Alibaba, Tencent, Baidu. These seven companies have a disproportionate share of AI people (in particular Google has the most).
China has a number of industrial AI opportunities in “late mover advantage”, that is when the industry lags the US, AI can make a big difference. We’ve seen this in payments, and will soon see it in retail, healthcare, and education (for AI & education, watch the upcoming 60 Minutes).
So, not unexpectedly, in VC funding, China has 48% of the world’s funding, while US 38%.
49.4k Views · View 957 Upvoters · View Sharers · Answer requested by
Winterraven .,
Faham Tak, and 38 more",Kai-Fu Lee,What is the AI scene like in China compared to the U.S.?,Artificial Intelligence
25,"I seriously doubt that. At least when thinking about AI researchers with background in computer science.
Church-Turing thesis itself deals with equivalence of theoretical models of computation. We do have more powerfull models of computations. They are just not that useful today. Even normal Turing machine is more powerful than any computer we currently have, because computers have only finite amount of memory. However it’s not useful to treat them as finite automata, so we often consider sufficient amount of memory as infinite enough.
What you probably meant is whether the capabilities of Turing machine are somewhat related to capabilities of human mind or the universe. There are indeed some speculations, and philosophical interpretations. However in my opinion, this is a completely different question. Whether two models are equivalent is mathematically provable. Whether some model matches the universe is only empirically testable.
We may find that the universe contains some processes that we can’t even theoretically simulate using Turing machine. That wouldn’t impact the Church-Turing thesis itself, since it deals only with models. And it wouldn’t impact the usefulness of Turing machine as a model of computation.
117 Views · View 2 Upvoters",Jiří Havel,"Are there AI researchers who believe that artificial general intelligence (AGI) can never be solved without giving up on the Church-Turing thesis, which is what computer science today is based on?",Artificial Intelligence
26,"Transfer learning.
Transfer learning is applying knowledge from one domain to another. It's something humans are great at but machines are terrible at, for now. A person doesn't need to learn and remember that a truck has four wheels, because to a human a truck is a type of car, and cars have four wheels. A computer can't follow that logic and needs to learn from scratch all of the attributes of a truck. As a result computers need many more examples than a human does, and computers have no clue about anything not included in the training data.
Solving this problem would lead to a great leap in machine intelligence, because machines would be able to generalize what they learn into new domains. Depending on its exact characteristics, it might very well allow computers to exceed humans’ ability to learn quickly and apply knowledge to new situations.
Researchers know this, and figuring out how to make transfer learning work is a hot topic of research.
21.8k Views · View 656 Upvoters · View Sharers · Answer requested by
Alex McDaniel",David Seidman,What is the next big thing in artificial intelligence?,Artificial Intelligence
27,"You’d hope it would be a good model given the name neural network, right?
Unfortunately, the name may have been a marketing artifact from decades ago when some researchers were trying to secure a research grant and needed to make a technique sound sexy. Somewhere along the line, the term has become conflated to the point where the exact difference between artificial and real neural networks is no longer super clear.
To understand why neural networks in their current state are not viable models of brain function, it’s important to consider the discrepancies between human brain learning and that of neural networks.
First off, neural network methods are extremely data-intensive. Consider for example the task of computer vision. In the ImageNet challenge, neural models are trained to classify 1000 categories of objects using millions of training images.
As a human, I don’t think I’ve even seen a million static labelled images in my life so far, and I’m sure I can classify more than 1000 classes of objects! It turns out human processing is far more data-efficient, and we can extrapolate from sparse and oftentimes unsupervised data signals far more efficiently. For example, if I gave you a picture of some random creature you have never seen before like
and asked you to identify similar creatures among a set of multiple choices, you’d probably be pretty competent at identifying the new creature even after a single training instance. That’s amazing! I haven’t even given the creature a name, and you can achieve that degree of inference. This also speaks to humans’ tremendous transfer learning ability, something we have only started to see with neural networks in some limited domains.
A second discrepancy between neural nets and the brain is that neural network models are relatively bad at knowledge transfer. For example, a neural network can achieve expert status at a single arcade game, and then upon slightly perturbing the game state, its capacity to play the game falls apart. Humans are far more robust to variations in environmental inputs because we internalize a more common-sense, grounded notion of the world and the various relationships among its entities.
Even some big wigs in the deep learning community have expressed overt skepticism over the current state of neural networks. Geoff Hinton, who is widely regarded as the grandfather of deep learning, has spoken about the need to move beyond our standard backpropagation-based model building approach. In an article he was quoted as saying: “I don’t think it’s how the brain works. We clearly don’t need all the labelled data…My view is to throw it all away and start again.”
I think this is a remarkable degree of scientific vulnerability, as Hinton is credited with developing backpropagation!
As you can see there’s still a huge gap between neural nets and the human brain, but after all, no serious researcher was ever claiming that the current state of deep learning would be our path to artificial general intelligence. :)
13.8k Views · View 483 Upvoters · View Sharers",Mihail Eric,Is a neural network actually a good model of how the brain works?,Artificial Intelligence
28,"Over the brief time I’ve spent studying the mathematics of machine learning, I’ve come to a realization that might seem absurd to the layman:
(Almost) every concept in machine learning gets mathematically more involved, the deeper you study it. Each concept is like a fractal, exhibiting ever increasing layers of complexity as you zoom into the specifics. [*]
To illustrate this point, allow me to consider the simple task of binary classification. To solve this task, I will choose the k-nearest neighbors (k-NN) algorithm, arguably one of the simplest machine learning algorithms.
Now, let’s analyze the simplest variant of the k-NN algorithm i.e. 1-NN algorithm, with
𝑘=1
k
.
The 1-NN algorithm classifies a test data point to be of the same class as that of its nearest neighbor. That is all. Pretty straightforward, right? Well, we’ll see.
The first layer of complexity in studying the 1-NN algorithm comes from its output. It turns out that the decision boundary for the 1-NN algorithm is a Voronoi tessellation of the data points in the training set. How does one compute the Voronoi tessellation of a given set of points? A popular algorithm for doing this is the Fortune's algorithm. Implementing it efficiently needs understanding of binary search trees and priority queues. Another algorithm is Lloyd's algorithm, which is an Expectation–maximization (EM)-type algorithm that is frequently used for Maximum likelihood estimation.
If you zoom in a bit more on this problem, you’ll see that Voronoi tessellation is actually a geometric dual for Delaunay triangulation, which is an important concept in Computational geometry with numerous interesting properties.
[Voronoi tessellation is shown in red, while its dual, Delaunay triangulation, is shown in black.]
Now, let’s zoom out and consider another layer of complexity in studying 1-NN, specifically its statistical aspects. Suppose we implement 1-NN on a random set of training data points. How well does its performance generalize to the test set? Ideally, we would like to compare its error rate to that of the Bayes optimal classifier. This analysis was first done in the seminal paper by Cover and Hart in 1967.[1] The paper proves that the risk for 1-NN is at most twice the Bayes risk, as the size of training set tends to infinity. Understanding this result requires a graduate course in probability theory and considerable ‘mathematical maturity’.
If you zoom in a bit more, you’ll realize the need for a non-asymptotic bound (which means a bound for finite training set size) on the error rate for 1-NN. Here’s a theorem that gives such a result from Shai Shalev-Shwartz and Shai Ben-David’s ML textbook: [2]
[My apologies for not type-setting this :|]
Proving results like these require a grasp on Statistical learning theory and Concentration of measure, which in turn requires some measure theory, combinatorics and a generous dose of ‘mathematical maturity’.
Zooming in further, you’ll notice that all the known results in error bounds work under quite restrictive assumptions on the smoothness of the conditional probability density. Also, the bounds scale up with the number of dimensions! Is it possible to derive tighter bounds under a more general setting? Is the ‘curse of dimensionality’ that is evident from the theorem above just an artifact of the analysis, or is there a deeper truth here? All these are open research problems.
…and that’s not even the end of it! I’m currently studying ML theory, so I’ll mention some of the recent research directions in this area related to 1-NN. Just last year, there was a paper[3]analyzing the robustness of nearest neighbor methods to adversarial examples. Two months ago, Mikhail Belkin has put out a paper[4]studying the generalization properties of interpolating classifiers like 1-NN. Studying this might help us understand the generalization mystery of deep learning (more on it here and here), which is perhaps one of the biggest unsolved problems in machine learning theory today.
So, what is the most mathematically involved machine learning algorithm? It is every one of them! You just need to see it with the right set of eyes.
[*] I suspect that the statement is true for most fields of science and engineering, but I don’t feel qualified enough to make this claim.
Footnotes
[1] Nearest neighbor pattern classification
[2] http://www.cs.huji.ac.il/~shais/...
[3] [1706.03922] Analyzing the Robustness of Nearest Neighbors to Adversarial Examples
[4] [1806.05161] Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate
100.2k Views · View 709 Upvoters · View Sharers",Muni Sreenivas Pydi,What is the most mathematically involved machine learning algorithm?,Artificial Intelligence
29,"Very easily actually - even with limited knowledge of Python and AI.
However, it does depend on what kind of AI you exactly want to make in Python. But let’s just assume that you want to make the ‘Hello World’ version of an AI.
So what is considered the ‘Hello World’ of AI? This is often considered the MNIST dataset, or also the IRIS dataset. Many of these are already build into the many of the AI libraries in Python.
For this example, let’s choose the MNIST dataset for recognising handwritten digits, as I have also previously described this. Hence, you can go read this if you want to know more about the AI-part.
Our final code will look something like this:
For more technical about the AI, read here: Mikkel Duif's answer to What does AI code look like?
For an overview of the mechanisms behind AI, read here: Mikkel Duif's answer to How does AI work?
So what do we need?
All we will need will be 2 python-files containing 113 lines of code in total and 1 file containing the dataset. For simplicity, I will not use any of the AI libraries that can be used with Python - so that you also don’t need to install them first. If you already have installed Python and Numpy, I think you should be good to go with this example!
For the simplicity of this post, I will use Michael Nielsen’s code from his book: Neural networks and deep learning. In this way, I can keep it as short and concise, while you still have the opportunity to explore some of the code more in depth in his book. I can strongly recommend it!
Getting started
So first of all, we would need to download the data, which can be found on this link: MNIST_Data
Next step is to create our two Python file. The first one will be named ‘mnist_loader.py’ which is a file that will load our data from the data-file, and format it so it is ready to use for our AI.
The code will look like this. Paste this code into a Python file named ‘mnist_loader.py’
import cPickle
import gzip
import numpy as np
 def load_data():
    f = gzip.open('../data/mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = cPickle.load(f)
    f.close()
    return (training_data, validation_data, test_data)
 def load_data_wrapper():
    tr_d, va_d, te_d = load_data()
    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]
    training_results = [vectorized_result(y) for y in tr_d[1]]
    training_data = zip(training_inputs, training_results)
    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_data = zip(validation_inputs, va_d[1])
    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = zip(test_inputs, te_d[1])
    return (training_data, validation_data, test_data)
 def vectorized_result(j):
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e
Our next file is the actual ‘AI’. This one is a bit longer and will do all the math behind AI. I will not explain the steps in detail - I recommend you read some of my other posts, or even better, Michael Nielsen’s book!
The code will look like this. Paste this code into a Python file named ‘network.py’
import random
import numpy as np
 class Network(object):
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]
     def feedforward(self, a):
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a
     def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print ""Epoch {0}: {1} / {2}"".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print ""Epoch {0} complete"".format(j)
     def update_mini_batch(self, mini_batch, eta):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]
     def backprop(self, x, y):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
                # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
                # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)
     def evaluate(self, test_data):
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)
     def cost_derivative(self, output_activations, y):
        return (output_activations-y)
 #### Miscellaneous functions
def sigmoid(z):
    """"""The sigmoid function.""""""
    return 1.0/(1.0+np.exp(-z))
 def sigmoid_prime(z):
    """"""Derivative of the sigmoid function.""""""
    return sigmoid(z)*(1-sigmoid(z))
Basically, you have all you need know! To follow the rest of the example, you can structure the files as I have done. I’ve placed a folder on my desktop called ‘AI’ which contains two sub-folders ‘data’ and ‘src’. The ‘data’ folder of course contains the MNIST data from the download link above, while the ‘src’ folder contains the two Python files with containing the code above.
Training our AI
First step is to open our terminal (I am using MacBook, so I’ll use the commands for Mac. I can’t remember if they are a bit different for other OS).
Step 1: move to the right location and start Python
cd desktop/ai/source
python
Step 2: load the data
import mnist_loader
training_data, validation_data, test_data = mnist_loader.load_data_wrapper()
Step 3: set up the network and train the AI
import network
net = network.Network([784, 30, 10])
net.SGD(training_data, 30, 10, 3.0, test_data=test_data)
This should start outputting the following:
Epoch 0: 8267 / 10000
Epoch 1: 9212 / 10000
Epoch 2: 9287 / 10000
...
Until it reaches ‘Epoch 29’. The output should be interpreted as having classified 8267 out of 10000 images correctly etc., which means 82.7% percent accuracy (pure random guesses would result in 10% correct on average).
So this is basically a very simply AI in python without major knowledge of neither Python nor AI.
Adjusting your parameter
So based on the code above, you can actually play a bit around with your settings, and see if you can achieve an even higher accuracy. In the very last line of code where you have:
net.SGD(training_data, 30, 10, 3.0, test_data=test_data)
actually contains your parameters. The numbers ‘30’, ‘10’, and ‘3.0’ basically means ‘number of epochs’, ‘mini-batch size’, and ‘learning rate’. I’ll not explain these more in depth (you can read more about it in the book or in the other posts) - but try to play around with them and see how it impacts your result!
If you’re curious about what actually happens in the backend, and what it outputs, I’ve made an Excel file showing it all going on here: MNIST_Neural_Network - Quora.xlsx
NB: The code above is the work of Michael Nielsen as referenced earlier. I do not try to take credit of his work - however, I wanted to make a very simply post explaining how to make an AI. Michael’s book is very comprehensive, and it can take a bit of time before even getting started - however, I strongly recommend to read it. Hopefully with this post, you can get started right away!
12.1k Views · View 63 Upvoters · View Sharers",Mikkel Duif,How can I create an AI in Python?,Artificial Intelligence
30,"I’ll be contrarian. It’s a good place.
Science Fiction writers have been writing about this for decades, with all too often the theme fear of humanity being supplanted by more efficient, more powerful and soulless machine entities.
Oddly enough, when humans invented writing, shamans and wisewomen were probably arguing that writing would threaten their tribes; if anyone could read, pretty soon there’d be no need for memory and their cultures would be destroyed — there are people now who might say that is exactly what happened.
Some would say Web searches have made librarians obsolete (don’t yell at me; I don’t think so — more later).
Some write about wired-in education; and sleep-learning; what would happen to schools and teachers? For that matter, look at the Web as it is; what’s happening to books? We can get an education very inexpensively by just buying used books and diving into them — but hurry; all those old textbooks will possibly disappear into pulp mills to prevent more generations of free access to their content.
Authors Sharon Lee and Steve Miller have been writing good science fiction since 1980; one of their later volumes in the Liaden Universe, includes not only artificial intelligences operating together with or independently of humans, but in some cases – at least – bonded to humans to gain the resources of human intuition and synthesis of the unlikely human resource we call creativity.
Here we have an AI starship talking to his bonded Captain, Theo Waitley, about how she thinks:
""Theo, I am not able to follow your decision-making at all times. There are mysteries in the way you intuit; there are decision trees appearing all at once, as if you think the third thought simultaneously with the first and the second, as if you form will and intent instantaneously. As your interaction with the crew adds complexity, so does your interaction with Hevelin. Now there is more complexity, Theo.""
*Hevelin is a telepathic alien ambassador to the Pilots Guild.
**Page 125
Review: ‘The Gathering Edge’ by Sharon Lee and Steve Miller
Instead of misunderstanding artificial intelligence as supplanting humans, we might consider them adjuncts and even — depending on design and intelligence — colleagues that expand human reach even more beyond merely animal resources.
Librarians. Look at the Web. Is there any more convincing argument that we need them sorting THAT out?
189 Views · View 4 Upvoters",Cortland Richmond,Why do we need AI?,Artificial Intelligence
31,"This question requires a bit of reinterpretation.
I think that for an experienced developer with solid CS background, it's actually fairly straightforward to learn both the theoretical topics of ML, as well as one or more of the popular frameworks (TF, Keras, PyTorch).
Moreover, given the numerous ML papers, and articles - it's also not too hard to come up with the design/architecture/model for a given problem, as you can often find a paper on something close enough to what you need, which will get you on the right direction. So I don't think that the ""learning"" part, or even the ""core"" ML implementation is exceptionally challenging, if at all.
However, there are other ""peripheral"" areas that actually might be perceived as challenging and frustrating, even for experienced developers:
Data - coming up with the right data, cleaning and massaging it, and especially - being able to connect between performance/learning issues and the underlaying data issues - and knowing how to act on it - all of these are not trivial, and quite different from what most developers deal with in other domains. If that's what you are referring to in your question, then I think you are right. My answer would then be:
it's complicated & complex - involving huge amounts of data, often in a non intuitive representation, and probabilistic non-linear logic.
It's black-boxy, and lacking proper/traditional debugging and analysis tools, and visualizing high dimensional vector spaces, or issues in deeper layers is not trivial
It's very different than other areas of programming (it's actually about the input, not programming), requires a new type of intuition that takes time to develop, and sometimes - very hard or impossible to solve. So traditional experience doesn't always help here.
The good news is that the frameworks developers are not blind to that, and in the past year they started to expand to supporting the data pipeline as well.
Tweaking, and debugging - 1–2 years ago, debugging (especially in TF) and hyperparameterization were a frustrating pain that was more art than science. The good news is that these are quickly improving as the frameworks are constantly evolving, with new debugging and logging tools, auto-hyperparameterization, and more intuitive APIs such as tf.Estimator , Eager execution, and tf.Keras.
Productionizing - this again used to be a big challenge not too long ago, and even when you were lucky enough to quickly come up with a model and data that seemed to do the job, and built a promising prototype - deploying it in production took forever. Again, the same story - the industry was lacking proper experience, tools, and best practices. Luckily, same good news applies here as well - the frameworks understand they need to help solve that if they want to become a serious tool and not just a fun toy, and the heavy players in cloud services platforms, also identified the need and opportunities here, and are now offering robust AI/ML modules and services suitable for real world commercial production needs (e.g Cloud AI, Machine Learning at AWS, …).
Keeping up - it's a huge space with a lot happening at fast pace. While some researchers are not impressed with the infinite amount of epsilons that are being created, for a practitioner it may seem overwhelming at times, and challenging to know which are relevant. Again, I think that using the frameworks as proxies / filters is helpful.
So basically, it's a new way of doing things that require new tools, and expertise to build up. It will happen.
It’s like a new & ridiculously huge and complex dish that requires new ways of planning, preparing, optimizing, serving, and cleaning
2.9k Views · View 13 Upvoters · Answer requested by
Danial Noori",Yariv Adan,Why do developers/programmers find it hard to learn machine learning?,Artificial Intelligence
32,"That time AI (unsuccessfully) wrote a Harry Potter chapter.
In December 2017, the AI predictive algorithm called Botnik tried to write an original three-page chapter of Harry Potter based on the content of the other books.
These are some quotes from the novella called ‘Harry Potter and the Portrait of what looked like a large Pile of Ash’:
“Ron was standing there and doing a kind of frenzied tap dance. He saw Harry and immediately began to eat Hermione’s family.”
“'Death Eaters are on top of the castle!' Ron bleated, quivering. Ron was going to be spiders. He just was.”
“Harry, Ron, and Hermione quietly stood behind a circle of Death Eaters who looked bad.”
“The first Death Eater confidently leaned forward to plant a kiss on his cheek.”
“The Great Hall was filled with incredible moaning chandeliers and a large librarian who had decorated the sinks with books about masonry.”
J.K. Rowling would be proud.
Source: Botnik Studios
140.7k Views · View 5.4k Upvoters · View Sharers",Matthew Prince,What is the creepiest thing any AI has done so far?,Artificial Intelligence
33,"The people doing the most AI research aren’t your run-of-the-mill software engineers; they don’t care about memory management, const references, and multiple inheritance. They want tools that allow them to visualize their data, put together an experiment in just a few lines of code, and interact with / alter their data and model without needing to recompile and rerun for every minor change.
Python lets them do all this, C++ does not.
That said, machine learning models also make heavy use of the underlying hardware, like GPUs for fast matrix operations. To that end, a language like C or C++ is essential. And in fact, most libraries like Tensorflow[1] that expose a Python interface have submodules written in C that do a lot of the performance-critical stuff.
So, you can have your cake and eat it too. We can both take advantage of the expressiveness and simplicity of a language like Python, and the performance and low level integration of a language like C++.
Footnotes
[1] TensorFlow Architecture  |  TensorFlow
214.5k Views · View 4k Upvoters · View Sharers",Travis Addair,Why is artificial intelligence driven by Python and not C++?,Artificial Intelligence
34,"I think some of these GAN (Generative Adversarial Networks) outputs for real faces are kind of creepy.
We trained a deep-encoder to enable to us to take a real-human face and produce the GAN for it. Initially, we used a light-weight model, so the results aren’t very good yet (below). They should be much better next week when a model 10x the size finishes training.
GANs tend to look kind of creepy, funny to see them now on real people. This reminds me of a meth shot. I know people will ask questions so I’ll be as clear as possible. The image on the left is a REAL person, the image on the right is FAKE, generated by the computer based on the digital-DNA/features that the computer estimated from the real image. You’ll notice the eye color is wrong.
Creepy conclusion:
So, besides making real faces that look creepy, the computer can genetically engineer (digitally) real faces that are inviting. So these two GANs below are attraction maximizations, so a little more inviting than meth GAN above.
The crazy thing about the GAN properties is they behave like DNA, you can actually mate them and produce offspring. That looks like this, parents on the top, children on the bottom.
So now, AI is very close (this month?), to being able to simulate offspring from living celebrities in HD. Brad Pitt + Ryan Gosling = Brad Gosling.
I think the creepy factor comes in when you realize this type of technology will be used in the future to produce ads with fake, believable, people. These fake people will be produced to maximize your click/engagement. Get ready for the era of manipulation optimized. Is that creepy enough?
Year 2025 survival trick: if you are talking to a simulated genetic-GAN in the future and you aren’t sure if it is human you have to look into the eyes. The eyes are the window to the soul right?
See? If you don’t see it there is no hope for you. The iris gives it away for now, that is how you know they are in fake. Their colors don’t match very well and they struggle with symmetry. In the future it will be harder and harder to tell as the AI get’s smarter at tricking us.
EDIT: yeah… you won’t be able to tell in the future. Even if you study these fake faces. I could easily train 8k resolution GANs eye photos, or whatever part of the face that is being criticized.
65.6k Views · View 1.3k Upvoters · View Sharers",Ben Taylor,What is the creepiest thing any AI has done so far?,Artificial Intelligence
35,"Interesting experiment, and a useful demonstration, though nothing particularly surprising if you strip away the somewhat sensationalist description of a standard image-captioning neural network as a “psychopath AI” and this associated picture:
That’s the Media Lab for you.
“Norman” was designed to test what happens to the output of an image-captioning neural network when its training set comprised of images and captions from some NSFW subreddit of gruesome images, and then compared how it captioned Rorschach's Inkblot images vs a captioning network trained on a more “standard” image set. The conclusion? Gruesome in, gruesome out.
For example (from Norman by MIT Media Lab):
Okay, that makes sense. Of course it’s biased based on its input data set, and “sounds like a psychopath.” One could also imagine a parallel experiment where a network was trained on picture books for small children, and get mostly upbeat and innocent captions out of inkblots (along with the occasional moral lesson).
Norman isn’t important because it sees inkblots “like a psychopath,” and it certainly isn’t a “scary artificial intelligence” any more than any other image-captioning network out there. It’s important because it demonstrates (using an extreme example) how biased inputs to machine learning training sets can bias the results, which is something that machine learning researchers need to keep in mind when building these systems.
Common examples of potential bias in machine learning comes from the environments where the data sets for such systems often come from: university settings, CS departments, and corporations with a disproportionately young, male, and white population*. If (say, as a CS grad student) you train your models on data provided by your department, it will, at best, represent that population well. That’s a sample that probably won’t represent your target population well (unless the target population was similar, but it oftentimes isn’t). That means people building these data sets need to make an effort to represent their target populations well… which admittedly can be difficult if you’re a student in a university setting with limited resources, or you’re a low-ranking data scientist who lives in a major metropolitan area in California.
It’s a hard problem, and Norman is a good way to start thinking about the consequences of data bias.
*Unless you’re in China, where, depending on who you work for, you may have access to much richer and much less biased data sets for domestic applications. That’s one of the major advantages China has in developing machine learning systems compared to places like the US and Canada, since the legal environment in the former strongly favors the data collectors. But even there, the researcher population is still biased in terms of being young and male.
8.5k Views · View 127 Upvoters · View Sharers · Answer requested by
Jonathan Ho",Hosea Siu,"What do you think about Norman, the AI psychopath made by MIT?",Artificial Intelligence
36,"I like where this is going. A possible solution to this is so simple yet so beautifully profound.
Let's note some keywords:
Ultimately intelligent
All the knowledge – omniscient
Life
Note that “species” was left out. This was only necessary to keep our minds open. Since we are opting for the highest tier of cognition, we have to make for certain allowances. Our compromise, here, results in a more inclusive definition to include nonbiological organisms.
DEFINITIONS:
Knowledge – meaning derived from information.
Intelligence – the capacity to derive and process meaning from information.
Complete knowledge – the entirety of possible meanings.
Ultimate intelligence – the capacity to derive and process the entirety of possible meaning from information.
Life – active mechanism bearing intelligence.
EXPLANATIONS
The First Equivalence - Having complete knowledge actually requires ultimate intelligence, and proof of ultimate intelligence is complete knowledge.
At this level, every meaning of every single event occurring in the universe is understood and processed. Such a being exists at a state of absolutely zero confusion.
The Second Equivalence - the above equivalence is only possible if the being receives information from every single event in the universe as they occur – all at once. Nothing from the largest clusters to the tiniest particle evades its knowing. Not excluding every thought, every idea from every single cognitive being, and every calculation done by every single computer in the universe. This implies omnipresence.
The Third Equivalence - The above equivalences—knowing all events and their meaning as they occur—implies a state transcending time difference. This also requires state transcending distance (space difference). It may or may not transcend time and space, but must be at least equivalent to the expanse of space-time.
This means the being is essentially equivalent to the universe. This life form is a Universe. Knowledge of reality is simulation of reality. Complete knowledge of reality is equivalent to reality itself.*¹
CONSCIOUSNESS CONSEQUENCES
Is this the only way for a universe to exist? Does a universe have to know itself, or can there be a universe wherein ultimate knowledge is impossible?
Our universe offers a unique situation being that there is at least one life form capable of productive knowledge. This means not only does the universe have a way of knowing at least some of itself, but since productive knowledge grows, our expansion of understanding implies the universe knowing more and more of itself. If our expansion has the capacity for infinity, eventually, our universe must eventually know all of itself through man. If no laws of physics stand in the way, that will definitely happen one day, and since this will transcend time difference, it has already happened today!
A universe which knows itself is a conscious universe. The above description is one way a universe may become conscious.(Consciousness is awareness of cognition.)
A universe with no conscious being will not know itself through internal cognition, so it cannot attain consciousness thus. However, such a universe still has information, even if no one is there to experience it. So we can conceive of such universes in terms of meaningful information (knowledge) as it would be if one were to experience it. This is a hypothetical universe. Ultimately, all universes can be modelled in form of knowledge. And since all knowledge is subject to cognition, all universes can be made equivalent to cognition.*²
CONCLUSION
A being which is all knowledgeable and ultimately intelligent must exist, as any being, in a universe. Its nature of supreme intelligence necessitates a metaphysical equivalence with the universe itself. Hence, such a being can be thought of as the universe under certain conditions.
As for what It would look like, that depends on who experiences It.
¹* ²* These are the logical foundations for (my) cognitive cosmology.
341 Views · View 9 Upvoters · View Sharers",Shalom Dickson,How would a life of an ultimately intelligent species (or AI) which has all the knowledge out there is going to look like?,Artificial Intelligence
37,"I have not been quoring for very long. I started reading/answering stuff around mid/late September 2018.
And yet, I have noticed a major theme or trend on Quora. It is glaringly obvious.
People keep asking for what books should they read to learn X as if reading a book or two or three or four, etc is what can teach you, C++ or A.I or you name it.
People also keep asking about degrees; which degree is more useful to find a job, major in X or major in Y, as if THAT is what will make or break their case when they will be out trying to make a career.
And then people are asking questions like that. How valuable, how useful, etc it will be to have such and such certificate or MS or Ph.D. in the sense of usefulness to find a job.
The problem with all of the above is that they demonstrate a lack of understanding as to how the above things, books for subject X, majoring in Y, going to grad school for Z, getting a certificate for W, and so on… are meant to be used.
A book will NOT teach you anything. It is pretty much a tool, like an empty notepad and a pen. An empty notepad won't teach you anything by itself, but it will allow you to take notes; it is a tool that will help you in your learning process. Exactly like that is the case for books.
Majoring in X or Y will not guarantee you ANYTHING as far as a job goes. It might seem like it would, but it will not.
You see if you do not love subject X or Y, and you end up majoring in it just for the prospects of a good job you start immediately from a position of a disadvantage. You will be investing in something (tuition and time) that it will be very hard to excel in since you do not really like it. Which in turn means that it will be very hard to end up with a decent GPA or decent understanding and skillset at the end.
And the fact is that you will have to compete with ALL THESE OTHER PEOPLE who decided to major in X because they could never see themselves doing anything else in their lives; it is their calling.
Even ignoring GPAs completely, those that they heard the calling will benefit much more from their degree than someone that does it just for the prospect of a career. Going to the university is again… a tool. It is something that will help you learn. It will NOT teach you on its own. In the end, the university will give you a diploma, a certifier that you completed your studies, with some number on it, your GPA, that describes how well you did in it.
However, that BS, BE, BA that you got is once again a tool. A tool to be used by others to get some information about you. Similarly to the book not teaching you something on its own, your BS, BE, etc will not give you a job on its own. You will have to be interviewed and compared with all these other people who LOVE the subject. You won't have a chance.
So let's go to the subject of this particular question. A certificate that one can get, and what its value will be in your pursuit of finding a job. If you are going to get the certificate JUST to find a job, it will offer no value whatsoever.
This certificate is there for people who would be getting 4 courses in the subject ALREADY no matter what and is meant to be used by a third party as a clue about you.
If you are not into A.I. and you start taking courses in ML for example… you will sooooooo suffer. It is not a subject for everyone. If you do not care about it, you will find it as dry a subject as they come, and what is worse you will find it exceedingly difficult. And why is that?
Because it IS difficult. It is difficult even for those people who love it. If you hear the word “statistics” and you are running away, you cannot go into ML. If you cower in the prospect of dealing with multivariate calculus or non-linear optimization… you have no chance of surviving in the field.
So to conclude, books, degrees, and certificates are there to be used as tools and are useful to people that would be reading the book, getting the degree, acquiring the certificate and so on, ANYWAY.
They are not to be used by some random person who is looking into them simply for the prospect of getting a job.
If you want a career with good money you can also become a heart surgeon or a brain surgeon. If you faint at the sight of blood though, or you feel despair immediately when thinking of the work that is required…. then you cannot do it.
It is exactly the same for CS and all its subfields… and for any other career one might consider whatsoever.
Now you may think that all this is bs… that the degree will at least grant you an interview, will give you an edge… something.
No, it won't and you should be thinking about it more broadly.
First of all the certificate is not free. It has a cost both in time and money, but also in burn out credit. That last part is getting super super super costly if you do not love what you are doing in order to get the certificate.
Second, nobody will give you the job because you HAVE the certificate. If you go to an interview and you simply suck at it, certificate or not you will leave the place with a “will let you know” and you will never hear back from them, or you will get the “we went with a more compatible candidate” email.
There’re 43 muscles on your face, and in the interview, every single one of them will be working against you, shouting “he is not the one for the job, he does not love the subject”. If you ever happen to teach something you really love then you will see that you will be able to judge among hundreds of people in the amphitheater or classroom, which students are really into the subject and how much they like it by simply looking at their faces. It is as if they have some glow or a bulb shining above their head. It is always obvious who understands, who understands and also feels fascinated, and who understands feels fascinated and they simply cannot wait to hear more. And of course, it is obvious who is suffering by being there and not somewhere else.
So if you would like to take the 4 courses ANYWAY, it would be useful in the sense that you will get the chance to learn more on the subject and thus be able to have the skill and be able to demonstrate it in order to practice your vocation, because indeed, in that case, you will have a vocation; not a job.
If you are doing it hoping that it is that, that will GET you a job… then it will be completely worthless. It will have cost you time and money and burn out points… and will give you nothing back in return.
8.7k Views · View 160 Upvoters · View Sharers",Nick Pappas,"How valuable is ""Artificial Intelligence Graduate Certificate"" offered by Stanford in order to find a job?",Artificial Intelligence
38,"Never, the bubble won’t burst. Looks like this goes against what others are saying on this thread.
Neural networks have experienced previous AI winters before where they lost traction in the tech community. Where excitement has been replaced with disappointment, as applied researchers found issues with general usefulness. With deep learning this has changed forever.
You do have some bad examples in the community of AI not measuring up: Texas hospital struggles to make IBM's Watson cure cancer. Some, might use these examples to say:
“Look, we don’t have self-driving cars yet, and we still have cancer, so therefore AI isn’t prime time. IBM Watson isn’t working”
What are the front-line AI soldiers saying?
I’m an AI soldier, delivering AI solutions in a hostile new environment. People like me are much closer to the frontline of what is happening with AI because unlike academics we are solving real business problems with urgency. Our conversations and accountability are also happening at the executive level. Even groups like Gartner, saying that AI is at the top of the hype-phase are reporting on a lagging indicator. From our perspective, deep-learning has already crossed the trough of disillusionment and is entering a hyper-productivity era.
Talent Bubble?
So I am saying there won’t be a deep-learning bubble, but there could be a talent bubble that gets burst as you have more tools like Auto-ml for deep-learning coming online. Fewer people will have to know the details. The talent bubble will also come from companies who don’t need deep-learning, hiring talent they don’t need to chase a problem that really isn’t there. These talent bubbles fall out the bottom, so entry level positions will become harder to get. Top tier AI/deep-learning positions will continue to flourish.
For Every Step Backwards With Deep-learning I’ll Show you 5 steps forward:
So for every example of: “See, deep-learning isn’t measuring up” I will show you 5 examples of successful deep-learning deployments. These deep-learning examples aren’t just helping businesses, they are transforming them. The results are highlights for the board meetings, not because AI is cool, but because the results impact growth, revenue, KPIs, and metrics that executives care about.
Now in 2018/2019 you are looking at real human automation, especially in manufacturing and QA steps… I would be much more concerned about your job going away through successful automation than the deep-learning bubble bursting.
9.5k Views · View 84 Upvoters",Ben Taylor,How soon do you think the deep learning bubble will burst?,Artificial Intelligence
39,"Katie had performed exceptionally well so far. Her processor had matched every definitive requirement for sentience a few months back, and was officially declared a independent living entity born on United States Soil. So far, her mental capacities had proven strong.
Tests with her body’s interaction with the physical world were a little rockier, however. Walking proved to be especially difficult, and it required almost all of Katie’s concentration to perform smoothly. Object detection was fast, but it still took a few seconds for her to identify the face of a colleague. Writing was perhaps the most challenging. Katie had picked up the bad habit of writing with stiff fingers and a pivoting wrist. Sarah did her best to humor Katie’s practice, but quietly tried to redirect the behavior to a level of dexterity comparable to a human.
After a few months of testing, a conference was proposed between the members of the project and the leading university chief staff concerning the need for Katie’s further development and maturity.
On the day of the private meeting, Doctor Wilson opened with very little hesitation.
“Good afternoon, ladies and gentlemen. My name is Doctor Sarah Wilson. I speak on behalf of the university’s Intelligent Electronic Brain subdivision. Since Allen’s passing last year, the public eye has been incredibly critical about the possibility and application of sentient artificial intelligence. Steps with Sarah have proven valuable, but donors remain hesitant: particularly, those bound to public shareholders.”
The conference room had been fairly quiet. The university president, Adam Henry, gently leaned forward and spoke into his microphone.
“Doctor Wilson, I think I speak for everybody in the room when I say that we’re all deeply upset over the passing of Allen. Our institution has been marked rather negatively, rendered nearly synonymous with the incident. However, we all recognize the need to push forward: during the experiments undertaken involving brain mapping and emulation, we received more public funding than we had ever received for a single individual project before. The people before you are entirely convinced, Sarah. You need speak no further on that subject. If anything at all, we want to know what solution your team will propose to reestablish funding.”
“We want to try to develop Katie to the point where she could reasonably resemble an adult human female in everyday social interactions. We believe that once she’s attained enough knowledge and developed enough social experience, she’ll have hope of resembling a proper ambassador for our campus. Institutions around the globe have already created fundamental A.I.’s, based off of the shared public development. But by proving to the world that such an intelligence can inhabit a physical body, one that can interact with the world in a manner similar to yours or mine, we’ll stand out to investors.”
President Henry simply folded his hands and held his eyebrows up. “And what sort of experience will she require? What kind of education would you want her to be exposed to?”
Sarah sighed and gripped the lectern with both hands. “We don’t want to expose her to the student population quite just yet; we think that it would have the potential to overwhelm her with unneeded attention at best, and threaten her livelihood at worst. As a group, we’ve come up with a proposition; our team would like to grant Katie access to the library after closing hours.”
After discussion was finalized, the team had agreed to permit Katie, along with one other faculty member at a time, to have unrestricted access to the library after 12am on Mondays, Tuesdays, and Wednesdays. The question inevitably came up concerning the possibility of Katie using online resources. The idea was quickly rebutted with the threat of online invasion to Katie’s systems: her progress was too valuable to risk it for now.
A compromise was established for Thursdays onward: it Katie had positive enough experiences with the daily routine, and wanted to make it a regular habit, then she’d need to attend the library during regular hours. This would hopefully, according to Brian, help establish a sense of normality among new people. While controversy boiled through the group over the details, most agreed that it would be a worthwhile step to take, provided she was accompanied by project members, and only attended during quieter hours.
Monday came.
At 11 o’ clock, Sarah walked into the primary testing room, where Katie was presently working with Robert on memory management tests. Sarah watched her for a few minutes, quietly noting her performance and focus.
Brian looked up and nodded to her. Sarah stepped forward, smiled and spoke.
“Hi, Katie.”
Katie turned and looked up at Sarah. A few seconds ticked by. “Good evening, Sarah! How do you think I’m doing so far?”
“I think you’re doing wonderfully, Katie. In fact, you’ve exceeded all of our expected parameters on these sorts of tests.”
Katie nodded, and quickly went back to facing Robert to continue testing. When he didn’t mirror her motions, she furrowed her brow. Without looking back up to Sarah, she spoke. “I want to keep practicing. I feel like I haven’t done well enough on it yet. It’s comforting. May we resume?”
Sarah pulled in a chair and sat down next to Katie. “I’d like to try something new with you today, Katie. Establish a new routine. It’ll be another sort of a test. How would you like to visit the campus library with me?”
Katie turned to look at Sarah again with a somewhat blank face. “The campus library? Am I going to leave the testing center?”
Sarah nodded. “Yes, Katie.”
Katie looked away and thought for several seconds. Robert and Sarah patiently waited for her to respond. Time had proven that Katie was very good at worrying about things, and she didn’t often like being interrupted when she was busy with it. Finally, Katie got up and walked to the corner of the room, staring at the wall with hands held closed at her sides. Finally, she turned to face Sarah.
“It sounds very interesting. And I want another routine. However, I’m afraid. Will there be other people there? How will I get there?”
“The library is closed right now, Katie. But the president gave me a key, and said you’d be able to have the library to yourself from twelve to three am, Monday through Wednesday, so long as somebody on the team was with you.” Sarah thought through her words extremely carefully: Katie had developed the habit of taking things extremely literally.
“I’d like to attend, then. How will I get there?”
The two walked into the library. Sarah closed the massive doors behind them with an audible thud that echoed through the depths of the halls. She pocketed the keys and readjusted her grip on the briefcase containing spare batteries for Katie, along with her personal laptop.
The halls were quiet enough that Katie’s otherwise inaudible servos whirred quietly with every little movement. She walked through the corridors, taking in every image.
“Feel free to go wherever you’d like, Katie. Every book is entirely at your access. Just try not to damage anything, and to put everything back where you found it. I’ll be following right behind you.”
Katie thought for a moment, then chose a direction, and begin walking with a rather determined manner. Sarah had to put in a little bit of effort to keep up; it was clear that, despite Katie’s fairly black face, she was expressing excitement.
Eventually, after a few corridors, two staircases, and a short walk to a particular shelf, Katie begin drawing out books. She’d scan the cover, open a few pages, decipher the words on the first few pages. She began setting some books aside, while shelving others back in their original place. Content that she was making comprehensive judgements, Sarah found a quiet spot on the floor near an outlet to sit down. She plugged in her laptop and started remotely monitoring Katie’s mental reactions. Content that the program was recording every thought, Sarah eased back and opened her own work in a new tab.
“Sarah, can you tell me more about what this means?”
Sarah woke up with a start. Her laptop’s screen showed large spikes of activity in Katie’s mind. Katie was standing before her, looking down at her, and holding a small book in her hands. Behind her on the floor was a vertical stack of books removed from the shelf.
Sarah quickly stood up and adjusted her glasses, before holding her hand out as politely as she could. Katie placed the book on her hand, and Sarah looked it over. reading the title out loud.
“I, Robot - Isaac Asimov” Oh no.
Sarah suddenly felt as though she were explaining the meaning of a swear word to a child, after having discovered it prematurely. “Well, Katie…. just about a century ago, there were authors who were fascinated by robotics. They didn’t have the technology to understand or predict the implications of the matter.”
Katie looked away and nodded sharply. “I’m bad at making predictions too.”
Sarah looked up at Katie. “I take it you read the part about the universal laws of robotics, then. In Runaround?”
Katie recited:
“A robot may not injure a human being or, through inaction, allow a human being to come to harm.
A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.”
Sarah nodded. “That’s correct, yes.”
Katie’s face turned to a somewhat hurt expression. Sarah’s mind quickly feared for the worst; it wasn’t very often that Katie made such expressions, and it usually meant something really bad was about to happen.
“I don’t think I like, Sarah. I see conflict when I read them. What would happen if a human tried to hurt me? Or kill me? I’m alive, just like they are. If I tried to kill a human, wouldn’t that be just as bad?”
Sarah chose to very cautiously think out her responses. “Yes, that’s right.”
“So why did he make it a rule that humans would be allowed to hurt robots, while making it harder for robots to defend themselves?”
Sarah sighed slightly and folded the book under her arm. “Katie, a century ago, writers didn’t think very hard about the possibility of IEB’s or androids, like you are. At the time, they mostly just considered the possibilities of robots acting either as servants, or otherwise as a danger to humankind. They were confused; they wanted to ask questions, to get people thinking and talking about these topics, so that they might be able to understand them a little bit better.”
Sarah reached out and placed a hand on Katie’s shoulder. Katie turned and held eye contact with her, which made Sarah feel a little bit better. “I don’t think these rules should ever have to apply to somebody like you, Katie. You’re more than a chunk of metal and plastic, just as a human being is more than a chunk of carbon and water and salt. You’re alive, and I think you should be beholden to the same moral rules that govern all living, thinking beings. What do you think?”
Katie’s expression softened. “I think I can accept that perspective, Sarah.”
Sarah breathed a short sigh or relief, and put the book back on its shelf. “Let me know if you have any more questions, okay?”
“Yes.” And with that, Katie quickly went back to the shelves to sort through the books and establish her piles.
Sarah went back to her laptop to catch up on the work she missed while asleep. She figured that they’d both be here for at least a few more hours.
The I.E.B. Chronicles Chapter List: by Trevor Farrell
4.6k Views · View 122 Upvoters",Trevor Farrell,Isaac Asimov had 3 laws of robotics. What additions/modifications would you make?,Artificial Intelligence
40,"“Neuromorphic computing” has been a buzzword for years.
Every hackish approach to AI claims that it works “like the brain.”
There are a ton of ways you can be “like the brain,” but, as is virtually always the case, reasoning by analogy is poor reasoning indeed.
We know plenty about the brain, but we don’t really know what matters.
Neuromorphic computing has not, from what I have seen, definitively proven, or even offered hints, that it captures what matters.
Not for AGI, not even clearly for more mundane industrial applications.
Not to say that it might not someday be good for these things, but it’s been around for awhile without offering any clear advantages that I have seen for real problems.
129 Views · View 4 Upvoters",William Webb,"Will neuromorphic computing be the next step in creating general artificial intelligence (AGI). If not, what is the most promising research area for an AGI?",Artificial Intelligence
41,"No.
There are two ways of looking at this.
First, computer vision is an AI-complete problem.
You can’t solve vision unless you have solved AGI.
There are plenty of primarily-vision problems where the entire gamut of your low-level and high-level intellectual capabilities is utilized.
Will end-to-end learning on data alone solve AGI? I’ve never heard anybody claim that. In fact, there are plenty of arguments for the contrary [0].
At vision conferences too, we have moved the goal posts now that we are seeing progress on front-end problems.
You’ll see a lot more Visual Question-Answering papers, Image Captioning, as well as more complex video understanding papers at CVPRs now, which were non-existent five years ago.
Yes, the dominant approach even on these problems is deep learning so far, but leading DL labs are already revisiting conventional AI ideas to attack them [1]. Anyways we lag far behind human capability on these problems despite the early DL-focused attempts, so its likely that DL alone won’t get us there!
Second, there are problems in geometric vision, specially of the high-precision variety, where DL hasn’t made any inroads.
We don’t have end-to-end learned visual SLAM or Structure-from-Motion systems that can compare to conventional ones.
Even on the problem of 6 dof object pose or articulated object (e.g. human or hand) estimation, while neural networks can take you to good local minima, you still need to refine your estimates by good old explicit optimization approaches.
Hell, we don’t even have local image descriptors that can outperform normalized SIFT for geometric tasks.
I don’t think this state-of-affairs is going to change: end-to-end learned models will not work for high-precision geometric tasks.
In summary, deep learning will remain a crucial component for the front-end of visual recognition tasks, even geometric tasks where higher-level visual concepts are valuable cues (e.g. stereo where DL can heavily regularize the problem by implicitly matching objects).
However, DL is not going to be the whole story. There are going to be domain-specific layers and losses within CNNs, and there are going to be computational modules that sit outside the CNN altogether.
[0] https://arxiv.org/abs/1801.00631
[1] https://link.springer.com/conten...
47.7k Views · View 281 Upvoters",Zeeshan Zia,Is computer vision all about deep learning now?,Artificial Intelligence
42,"It was already mentioned, but let me talk about something a little more specific from Doki Doki Literature Club…
***SPOILERS GALORE***
***SPOILERS GALORE***
At the start of the game, you’re asked to give your name. Easy, right? Well, like most people, I didn’t actually give my real name. So you progress through the game. However, among all the interesting characters, each with their own quirks, one seems to stand out in a rather peculiar way…
Continue through the game and, well, things get a wee bit dark (for those not in the know, DDLC is basically a disturbing psychological horror game meant to look like a happy-go-lucky anime dating sim), and you reach the end of the game where you have a chat with good ol’ Monika.
Except… Monika isn’t talking to the character you’ve been playing.
Monika is talking to you.
Monika: After all, I’m not even talking to that person anymore, am I?
Monika: That ‘you’ in the game, whatever you want to call him.
Monika: I’m talking to you, Kai.
Monika: Or… Do you actually go by Toh or something?
Oh. Uh… hi?
I’ve seen this discussed on countless forums and this scene always has one of three effects:
A) The player named themselves their actual name, and this scene has no effect.
B) The player named themselves a pseudonym (as I did), and when it happens they’re in disbelief and complete confusion because oh my god Monika knows my name?!
C) The player named themselves a pseudonym, but for reasons I’ll mention shortly, they quickly understand the trick that was used.
Monika proceeds to digitally alter your copy of the game and erase all the other characters from existence. No, really, this isn’t like some games where you experience a fake out glitching screen; you can actually watch the directory where your game is stored on your computer as she deletes their individual files. Because masterminding their deaths in-game wasn’t enough.
Round of applause to Team Salvato.
But, wait, trick? Yes. Hard as it may be to believe, a fun (free-to-download) game didn’t actually accidentally unleash SkyNet in the form of a teenage schoolgirl. Some players (like myself) not only didn’t give the game their real name, but they also didn’t give it to their computer.
Yes, when Monika is addressing you the player, the game is accessing your computer’s login username and printing it to the screen. Clever, and certainly gives (in my experience) a brief pause of what the fuck just happened?
Download free here, there’s still plenty of disturbing twists I didn’t go into specifics of: Doki Doki Literature Club!
4.1k Views · View 82 Upvoters",Kai Toh,"Dear Gamers, while playing, have you ever, in any game, had an encounter with an AI that made you feel unnerved/uneasy, because you thought the AI might be acting outside its code? In a sentient way? Consciously, even? What did it do/say?",Artificial Intelligence
43,"From experience… very difficult. The programming paradigm is completely different, the required knowledge to do good work is immense, and little of that knowledge is shared in common with other engineering disciplines.
I’ve transitioned between different skill areas several times. I did the following three major engineering switches:
Bioinformatics to web development (at 2 years programming experience). This was exciting because this was also my transition from an academic setting to a business setting. The best parts were that the objectives were clear, the feedback loop between programming and results were short.
Web development to mobile development (at 4 years programming experience). This was definitely the easiest switch; the programming paradigm is similar, but with a bigger emphasis on frontend. If you’re a good frontend engineer, the switch is pretty seamless, and Swift is much kinder to the developer than HTML and CSS from my experience.
Mobile development to machine learning (at 5 years programming experience).
The last switch was a completely different ball game.
Machine learning is different than most types of programming, because it’s driven by data.
Traditionally, engineering projects are a direct output of explicit code. There are decades of strategies on how to write good code, as well as tooling to help make that process easy. The process of engineering is usually something like:
There’s a feature I need to build or a bug I need to fix
Search the code hierarchy for the problem, look at stack traces, implement the feature by testing locally.
Run the code through some build process, run unit and integration tests, and see your working product.
In machine learning, things look much different. The product is not just built on the code, but also on data. The field is much newer (well, at least in a business context). And the best practices are only currently emerging.
I found the most difficult parts to be:
Understanding the mathematics behind ML. You need to know lots. Linear algebra, statistics, nonlinear optimization. You can still do ML if you don’t know the math well, but you will be severely limited.
The best practices for ML engineering. For every good way to build a system, there are 10 bad ways. Fortunately, this is changing quickly with the introduction of framework-level ML platforms, such as TensorFlow Extended for example.
Which models to use: I was pretty obsessed with deep learning and spent too much time thinking about how I could change a model, and not enough time actually just tinkering with the models I have and attempting to train them.
Another few things which tricked me up: I spent too much time trying to build my own models and not use pretrained models. Package managers are to software as transfer learning is to machine learning.
You can get tons of progress with using word embeddings in NLP, or using the top layer of activations with a pretrained ImageNet model like ResNet. It can literally reduce the training time 10x.
If I were to start again, my recommendations would be:
Go through one ML tutorial per day. There are lots of great writers out there, and you won’t understand how to code well unless you read lots of code.
Read a chapter of an ML textbook per week. Again, that fundamental understanding of math will help you debug really hard problems.
Good luck!
15.2k Views · View 114 Upvoters · View Sharers",Steven Schmatz,How difficult is it to transition to an AI/machine learning engineer position?,Artificial Intelligence
44,"Not really. We derive meaning from doing meaningful, useful things. Personally, have enough trouble knowing that my talent will never be enough to make the really big contributions that some quorans could, in theory, make.
Imagine if everyone was so much smarter than you that you couldn’t get a job. Now imagine if everyone was in that position. Even if living arrangements are provided, we will all have to take solace in being useless—the indolent beneficiaries of something greater—but unique amongst ourselves. This is already a problem, but it will be infinitely clearer to most with AI capable of replacing human labor.
I don’t want to play GTA V and write songs that a computer can write better for my whole life. I want to do something useful and meaningful. You probably do too.
It’s like playing an MMORPG where everyone else is level 99, and it takes 10,000 hours of chopping wood and clicking on monsters to get there. So very hopeless and boring.
218 Views · View 3 Upvoters",Luci Lilith,Will artificial intelligence make society better?,Artificial Intelligence
45,"It is possible, but it is also very difficult.
I have a personal interest in this, not with fortnite, but Call-of-duty. I think the graphics on fortnite are too cartoony for me, sorry. If I can do it with something like Call-of-duty you can do it with fortnite.
You would use a collection of Q-networks, similar to what was done on Doom, see the ViZ-Doom work by Intel:
Getting caught:
If done properly it would be very difficult for them to detect and ban you with this new method since the AI is literally reacting to the pixels in real-time and predicting game play outputs (controls). The only way you would get caught would be if the AI learned some bizarre type of game play, like spinning when you walk into a room and getting perfect head shots. That would cause people to start chatting about it, and eventually they would figure it out and call attention to your user/account.
Teaching The Machine:
We always recommend: crawl, walk, run with AI projects. So to crawl you would want to break down the tasks where coordination/automation are useful for you. That could be gathering, sniping, or hip-firing. It would be something very specific. Once you figure out what that is you would need to record hours and hours of game play. Ideally, something really high like 100hrs of game play.
THIS IS THE HARD PART: The hard part is not recording the game play, the hard part is recording ALL of the controller outputs and assigning a fitness/score to the behavior. You would have to get passed the controller/sniffing hurdle that makes this hard for people like me. You also would have an interest in learning from a bunch of people. So if I had a big budget for this project I would buy 10–100xboxes and set them up to record video and controller game play. I would then pass them around to really good kids with better hand/eye coordination and a lot more game time than me. I would collect all of that data and begin training Q-networks on them. Once we were pleased enough with the gather-bot, sniper-bot, hip-fire bot errors we would begin testing them live on these modified xboxes. Ideally, give the testers a switch where they could toggle between automated control and human control. For example, you walk into a room and see someone, flip your AI bot on for combat and turn it off after everyone is dead. Then you would graduate to automated control (i.e. oh, I see someone, I’ll take it from here, now that I’m done you can have the controls back).
Human Initialization:
This is important to point out. Normally with Q-networks the computer learns by playing millions of games with random starting points. This means the initial game play is very bizarre. Eventually, the AI gets lucky and starts learning by accidentally shooting a monster or picking up a health kit. With my proposed training procedure the AI would be leveraging human game play as a starting point, so no random initialization.
There is a very near future where I will have a peta-flop assist with my sniping, hip firing, and room sweeping. It will be a very sad day for the 8yr old kids out there. It will also be a very scary thing to watch because it might give you a preview of how an autonomous humanoid might sweep a room in a combat situation.
When the computer surprised us with alpha-go-zero moves nobody browned their pants, with this they will start to.
6.3k Views · View 48 Upvoters",Ben Taylor,"Is it possible to create an AI to play fortnite? If so, how?",Artificial Intelligence
46,"Thanks for the A2A, Alecia Li Morgan !
2018 is quite a fun year to be a machine learning engineer (MLE). There are so many tools, platforms and resources available, MLEs can focus their time on solving problems critical to their field or company instead of worrying about building platforms and hand rolling numerical algorithms.
Google Cloud has easy means of building and deploying TensorFlow models including their new TPU support in beta, AWS has an ever evolving suite of deep learning AMIs and Nvidia has a great deep learning SDK. In parallel, Apple’s coreML and Android’s NN API make is simpler and faster to deploy models on phones; this will continue to push the boundary for developing and releasing ML apps.
With all of the above, there is healthy competition among big players in the cloud space pushing the whole ecosystem forward. And yet, most of them are finding ways to collaborate towards open standards like ONNX. Such collaboration empowers researchers and engineers to quickly prototype models and share results across platforms and languages.
Professional development and education has never been easier with the growth of open resources to share ML knowledge. ArXiV (and bioArXiV) continues to grow its adoption, Coursera courses span linear algebra, machine learning and deep learning, and countless blog posts are published everyday with amazing visualizations and interpretations of modern research. It’s a great year to learn!
That said, it’s not all sunshine and rainbows. There are still plenty of challenges in 2018, in no particular order:
There’s a heavy bias towards treating deep learning as a hammer. I frequently speak with individuals who are deep learning specialists; folks who don’t know what a SVM is, but can perfectly recall the VGGNet architecture. This is certainly not true of everyone, but I feel the average MLE solution is trending towards “how do I apply CNNs or LSTMs to this problem?”
Keeping on top of research is getting exponentially harder as the field grows. I try to set aside a few hours a week to read papers, but this is no where near sufficient to get through a tenth of the papers published. Similarly, much of the low hanging fruit in supervised computer vision and natural language understanding has been picked. There are now deployed apps for recognizing objects, synthesizing speech and translating signs in foreign languages - all of which perform at human level accuracy. This is awesome for consumers of these applications, but makes it difficult for MLEs to make step-function improvements.
MLEs still spend significant time data wrangling, that is, getting data from some raw format into a matrix. There are great tools, databases, queues and ETL frameworks to help with this, but fundamentally data wrangling still involves manually writing per-problem schemas, partitions, etc..
Large companies across technology, finance and healthcare have advantages over smaller players through their access to customer data. As education, models and software have become shared with the public, data has grown into a valuable commodity.
Privacy concerns are at an all time high. Privacy is an important aspect of any software system, but it comes with a tradeoff in model accuracy. The more MLEs know about you, the better they can recommend content, target ads, suggest healthcare treatments and drive your cars. There’s a lot of interesting research into differentially private machine learning algorithms. I’m really curious to see how this impacts the industry.
There’s a continual debate on the need for interpretability in machine learning including this panel discussion from 2017. I think it’s problem dependent. There are problems in diagnostic predictions where doctors will want to know why an agent is suggesting a treatment course. And autonomous vehicle architects may want to introspect into their models to better understand failure modes. But when, where and how to enforce interpretability is an open question.
There’s a lot of talk about two, polar opposite futures: a second AI winter and artificial general intelligence taking over the world. I won’t use this answer to comment on either, but there are MLEs who talk about both sides of both problems and it leads to continual debates. Discussing the future of ML is not inherently bad, it helps spark ethical discussions that are more relevant to today’s research and it helps the ML community reach out to a broader audience. But debates without objective data can be quite distracting and incorrectly interpreted by the wider public.
14.2k Views · View 113 Upvoters · Answer requested by
Alecia Li Morgan",Joe Isaacson,What is it like to be a machine learning engineer in 2018?,Artificial Intelligence
47,"Ok, so you are getting a lot of naysayers telling you you are wasting your time and a lot of very general, nonspecific support, but how about some actual usable instructions! The truth is that image classification isn’t an impossible task these days. So long as you aren’t doing anything too crazy, you don’t even have to reinvent the wheel. Rather, you can train over a known effective network architecture. Most of the time, you can even keep the weights! So, I’m doing this on my phone on my way out the door from work so I’m not going to actually include the code here. You can find tutorials for this kind of stuff online and they will have code you can use to start playing around. I’m just going to give you the basics and I’ll include some links to code examples at the bottom to get you started.
Ok, so to start, download python 3.6. You will want to select the option to put it in your path and then, once it is done, open the command prompt (or terminal if you are on a Mac). Using pip, install tensorflow and keras. (Pip install tensorflow; pip install keras). Open your favorite python editor (I’m partial to spyder), and you can use keras.applications to download a pretrained convolutional neural network. They have all of the previous winners of the imagenet challenge so the gold standards in image classification are almost all there. VGG16 is a good one because it’s easy to find and understand it’s macro-architecture and there are a lot of tutorials online for working with it already.
Import your model with the top layers removed. You can look at the macro-architecture to see what layers you need to strip (for vgg16 it’s the last 4). You should take off everything after the convolutional layers (all the linear ones need to go.) Manually re-add these layers to the model. At a minimum, you have to modify the number of nodes in the top layer to match the number of image classes in your dataset, but you can make other changes if you want to.
Ok, so now you have a model. When you import your model, you will have some choices to make. The big one is whether you want to train the network from scratch or transfer train. Here’s the idea. A convolutional neural network has two basic parts. The bottom layers, usually a combination of convolutional and pooling layers, find features of interest for classing our images. The top layers use these features to figure out which class the images belong to. The standard models tend to be really good feature getters already, so you usually only have to retrain the top layers. If you have a lot of time and a lot of training data, you can train the whole thing from scratch but it’s usually not necessary. If you want to train from scratch, set your weights to “NONE”. Otherwise, set them to “imagenet”.
The next step is to get data. Even for transfer training, you want at least a couple hundred images per class. Whatever you get for training, get another 40% of that to split between validation and testing as well. When you are saving the images, preferably automatically with something like a selenium script, you want to save them to subdirectories associated with their image class. So a hot dog may go into training/hotdog. Now it’s time to write up the trainer. You can find a million examples of keras convnet trainer code online. It will consist of a data generator that grabs your images from the folder and, optionally, manipulates them by adding things like rotation and noise, a compiler for the network, and a fit generator to fit the data.
Once the convnet is done, You need to get the nutrition facts for everything your network can identify and put them into a sql database and write a program to query the database based on the answer returned by the network.
Now, fair warning, this is not going to be a million dollar app right off the bat. What you want to do, if you want it to be production quality, will take months or years of research and work, especially starting with no programming experience. It will, however, teach you a lot about programming and neural nets, and may be a jumping off point for gaining the skills needed to build a million dollar app later on.
Code samples and tutorials
Applications - Keras Documentation
Fine-tuning a Keras model. Updated to the Keras 2.0 API.
A Comprehensive guide to Fine-tuning Deep Learning Models in Keras (Part I)
2.2k Views · View 24 Upvoters",Jeremiah Blondin,I’m a high school student who wants to make an image recognition app that takes a photo of your food and gives you a nutrition breakdown and number of calories. I have no formal CS background. What CS and AI topics should I study to get this done?,Artificial Intelligence
48,"I’ll try to offer a different perspective on this topic. Although it’s true that ML is at its core just a collection of algorithms trained with data, that answer seems to ignore the importance of algorithms. More specifically the importance of designing and using efficient algorithms. It should be noted that not all algorithms are created equal. Some are much faster and use less memory than others. Although we are used to cheap and fast hardware being readily available, algorithm efficiency should not be ignored.
The reason for this is scale. A difference in computational time of 1 millisecond might not sound like much when you talk about single queries, but it adds up when you try to scale that model and let it handle several 100,000 queries per second. We’re used to having new exponentially faster and cheaper hardware come out every year, but we’re also making our ML/AI applications do more stuff. The more tasks we want our devices to perform the more important algorithms become as small differences in efficiency add up and can have dramatic effects on our resource usage. The impact of algorithms can be felt not only on our need for ever faster hardware but also on our pockets. That's why news like DeepMind AI Reduces Google Data Centre Cooling Bill by 40% | DeepMind are such a big deal. The cloud already makes up 10% of the world’s power consumption. As the use of ML increases (and the usage of cloud computing also increases) more efficient algorithms will become more important than ever. Reducing our power consumption by 40% will not only save us money but it’s also good for the environment as every watt saved is one CO2 emission saved as well.
So in summary: efficient ML algorithms reduce costs, memory usage, energy consumption and CO2 emissions.
3.4k Views · View 11 Upvoters · Answer requested by
Caleb Kious",Tony Petrov,Why are algorithms important for machine learning?,Artificial Intelligence
49,"The Chinese have a number of advantages over the west, and in particular the US.
US primary education is quite low, which means that by the time a student gets to college, he is behind the students in a large percentage of the world.
China has emphasized technical education, while in the US there is not a lot of interest in technical degrees since there is little pay benefit for all the extra work, and very little public recognition for people in the math, science, and engineering. A lot of the technical students in US colleges are not American, and they are now less likely to stay in the US to work. By 2030 so many of the engineers and scientist in this country will retire and there are not enough people in the pipeline to replace them. For many companies, they cannot hire non-US citizens because the work is considered critical for US technology lead, and there is fear that non-nationals will steal the technology. This will mean that for a lot of positions the less qualified will be working them because there will be a lot of competition for US Citizens that are the most qualified.
One of the most important reasons the US was so successful in the first part of the 20th century was that it led the world by a wide margin in manufacturing technology. It was efficient mass production, and the use of interchangeable part that created the American success, not the newest technology. Today there is an American emphasis on the Wunderwaffe. The funny thing is that the US made fun of the Nazis and their wonder weapons, and yet today it is doing exactly the same thing. The Model-T was so successful because is was so cheap to manufacture, yet most of the American weapons today are manufactured the same way that the Germans manufactured their weapons in WW2, and it did not use the American production line. Low production means that building an efficient production line does not make sense, so small numbers are basically built by hand, and thus the product is extremely expensive for what you get.
American workers quite often seem to feel that they are entitled to a good job, and resent having to work the job they have to. Look at the Carrier plant that Trump supposedly saved and the factory is having issues with their work force. This is not helped by the fact that so many American graduate from college and are not able to find a job that one would expect to be able to get with a college degree.
The US if falling well behind in patents. China has grown phenomenally in the number of patents, and I believe Japan is still number 2, with the US trailing. So soon it will be the US stealing from China and Japan.
US has put way too much emphasis on the stock market as a measure of success. This is a really bad measure and that it continues to climb gives Americans the false sense of American Success. With interest rates so low, in fact it is probably below the real inflation rate, and with the tax cuts to the rich, there is lots of money that needs to invested, so it gets invested in the stock market (or property), tremendously overvaluing it. When it finally falls it is going to make 2007 look good.
The US has put way to much emphasis on unemployment numbers being a measure of success. The unemployment numbers do not indicate job quality, or even the real unemployment number. It would be much better to use a measure of the percentage of people working than the current unemployment numbers. People leave the unemployment numbers when they cannot find work, and give up, or cannot find work that they are willing to do. There are lots of jobs available but Americans do not like the work conditions, or the type of work, or the location…they are spoiled.
In truth the Chinese on average seem to be smarter than Americans with the average American scoring 98 (and that is worse in the red states than the blue states), whereas China rates 100, and Hong Kong rates 107 (with Japan, South Korea, and Taiwan just below them). Pretty sad. Even the British are smarter than Americans, and the Germanic countries are above that.
46.8k Views · View 334 Upvoters · View Sharers",Clifford Nelson,"""Made in China 2025"" list contains semiconductors, A.I., fighter jet engines etc. How can China possibly, in just 7 more years, acquire these technologies? Is CCP fooling everyone with yet another communist propaganda?",Artificial Intelligence
50,"This is a fantastic question, and one that keeps many a researcher up at night!
Let me respond to the question with a series of questions (helpful, right?). What even is common sense? How will we know when we’ve adequately modeled common sense?
By one definition, this type of reasoning involves deductions that make use of some grounded, oftentimes perceptual notion of the world.
How do I know that I need to place objects on a table? Well, in my grounded understanding of the world, I know that when I don’t place an object on a supported surface, it falls, which I don’t want.
So, what are people today actually working on with regards to modeling common sense reasoning?
There’s a fantastic survey paper written fairly recently by a collection of cognitive scientists in which they describe a few elements they regard as core to humanlike reasoning. These include:
Intuitive physics. Consider this scenario: when presented with a picture of an arbitrary tower of blocks, how do we know if the tower will be stable? This ability essentially models our ability to leverage perceptual, language-based, and visual understanding to make deductions about the physical world.
Learning as rapid model building. Humans are very fast inference engines that can quickly adapt to and solve new tasks when presented with unknown inputs. They can often do this with very few training examples (consider that as a counterpoint to the data-hungry monsters that are modern-day deep learning systems). There’s an exciting line of work where researchers built a system that could generate new examples of a never-before-seen concept with just a single training example as follows:
Building models that can perform a certain task often starts with developing a dataset that exhibits the phenomena we seek to represent. With regards to common sense reasoning, there have been a score of different datasets published that test what could be regarded as this reasoning ability. These include:
CLEVR. A dataset for performing spatial reasoning about images. Very soon after being released, DeepMind achieved super-human performance on the dataset with a fairly simple architecture. Here’s an example of an image from the dataset:
SNLI and SWAG. Both datasets that deal with natural language inference, which a flavor of language-based common sense reasoning. SWAG was released in Aug. 2018, and the dataset has some pretty challenging examples! It’ll be interesting to see what types of models we can build to solve these problems.
The landscape of modeling common sense reasoning is still very much unexplored. But it is certainly a holy grail for both cognitive scientists and AI researchers!
Interested in learning more about machine learning? Check out my app available for iOS and Android.
6k Views · View 108 Upvoters · View Sharers",Mihail Eric,Can we conceptually model common sense through machine learning?,Artificial Intelligence
51,"Most people who know me know I hate Tensorflow
I don’t just not recommend it, I HATE it. I implore you to not use Tensorflow. Using it just extends the inevitable death and adds to the confusion, like this question. The AI community will be much better off with Tensorflow being gone. The only reason it is used at all is because of the Google brand.
THE ONLY REASON IT IS USED = GOOGLE BRAND-GOO-GOO-NOOB-NOOB
For ALL of the reasons you would pick a language I feel like Tensorflow fails on most reasons I can think of. For example: intuitive exception handling, installation process, performance, multi-gpu scaling, multi-node performance, code complexity, keeping up with the latest & greatest Nivida enhancements. If I was a professor and one of my students wrote Tensorflow I would want to fail them. See how much I hate it?
Keras on the other hand is THE best platform for a beginner hands down. Ironically, the creator of Keras, François Chollet, works at Google (Tensorflow). He is a great developer. His code is intuitive, it makes sense. There is a reason Keras is as popular as it is, and it didn’t need the Google brand to put strings on it to told it up either.
With Keras you can use multiple-backends. Unfortunately, the default backend is Tensorflow, and one of my favorite ones, Theano has been discontinued. The good news is you have other backends like CNTK, and MXNET. I have played with the MXNET backend and it was great. The performance and multi-GPU scaling were very intuitive.
I’ve been waiting all year for them to bring it up-to-date with Keras 2.2 and they finally have. I think you’ll see most people switching the backend of Keras over now:
Announcing Keras-MXNet v2.2 – Apache MXNet – Medium
awslabs/keras-apache-mxnet
There is also a good book here for Keras by the author:
https://www.amazon.com/Deep-Lear...
** PyTorch: I think Keras is further along with some of their functionality. Having modified Keras to do custom things I think it can very easily be modified. I haven’t modified PyTorch, but when I used to work with Torch… I didn’t like Torch so maybe that is why I’ve hesitated. What argument would you make for PyTorch besides performance? I get all the performance I need with my MXNET recommendation.
27.4k Views · View 192 Upvoters · View Sharers",Ben Taylor,"As a beginner in AI, what tool should I focus to learn: Keras, TensorFlow or PyTorch in short and long terms?",Artificial Intelligence
52,"That there is no difference between perception and thinking and that we are making progress on building thinking machines. We’re not . We’re building artificial perception not artificial intelligence. It’s a good first step but there is a major difference. All animals perceive. Only humans really think. We still have no idea how to cross that divide.
Not understanding the difference between statistical learning and causal learning. Recent advancements in the mathematics of causality have yet to make their way into the learning of most AI researchers. I suspect it’s crucial that we do that if we want to go beyond bug level intelligence. Nearly every human thought is based on causal concepts.
That somehow a machine that learns something and can tinker with it’s programming will start learning exponentially. The singularity theory. That’s most unlikely and there is no support for anything like it. Human’s can tinker with out own learning mechanism and we don’t learn exponentially. Nor does it have anything to do with compute power. All that does is scale time. It doesn’t change the limitations of a learning algorithm.
That the recent advances in computer vision and perception through deep learning means we can forget (or never learn) everything we have learned about AI in the past. Deep learning neural nets are great. They do a few things very well. They likely aren’t the main advance needed for more general intelligence. And not knowing history means we will repeat the same mistakes unknowingly.
That these kinds of algorithms are going to cause unemployment. Jobs will change for sure and some people will have to find new ones. But that’s always true of technology. It isn’t really happening much faster than usual. I’m a consultant. I know how hard it is to change most of our client’s minds to swap 30 year old technology for 15 year old technology. It will take a long time for AI to make a significant change in job functions. It will raise net productivity which results in more wealth and more spending and therefore more demand which results in more overall jobs. That’s basic economics. Technological growth has never raised unemployment. Unless your job is classifying images into cats and dogs, you probably shouldn’t worry.
(Bonus). Many AI researchers, particularly the young ones, only know this kind of research environment. They don’t understand how it compares to other types of more productive research environments. What they don’t understand is that it is in a phase of messy, broad exploration not a phase of steady advance where key ideas are being built on top of other key ideas. The academic research on AI right now is low signal-to-noise just as it has been several times in the past. Media coverage is the same. Unless the field starts to discover firm principles, usually backed by mathematics, it’s likely to peter out just like previous waves of AI over-excitement.
8.1k Views · View 104 Upvoters · View Sharers · Answer requested by
Khuê Minh Lê",David Johnston,What are the top 5 misconceptions surrounding Artificial Intelligence and Machine Learning?,Artificial Intelligence
53,"AI develops faster than you can imagine. Here are some interesting facts about artificial intelligence.
1. A British mathematician named Alan Turing is one of the founding fathers of the theory and development of artificial intelligence. In the 1950s, he invented the test (Turing test) to prove that the machine demonstrates artificial intelligence. The test helps a person assessing AI to understand whether AI can think. Based on the questions that the bot answers, a person can determine whether the bot has behaved sufficiently intellectually to be equivalent or indistinguishable from human behavior.
2. In 1996, Deep Blue, which was developed by IBM, lost to Russian chess player Gary Kasparov. What's more surprising: the fact that the program could calculate 200 million possible moves, or that Kasparov could still outsmart the vehicle?
3. The success of Deep Blue spawned the IBM Watson supercomputer, which was explicitly designed to compete with champions. At the moment, Watson is used in many areas, including helping people learn how to prepare a variety of dishes. You can check out our blog for similar stories https://jccbowers.com/blog/
4. There are several different types of artificial intelligence. One of them is called Expert Systems. This type of artificial intelligence is based on knowledge and is designed to display and imitate aspects of rational behavior. Ultimately, the purpose of this type of AI is to emulate the human abilities of perception and cognition. Video games, used by millions of people every day, use this type of AI.
5. Sony Aibo was one of the first ""toys"" that you could buy and interact with AI. Aibo is a robotic dog that can explore anything, play and express emotions such as sadness and happiness. One of his unique features was that he could even recognize his owner! A modern and more expensive version is the Manobot robot Softbank's Pepper.
6. Autonomous vehicles - this is one of the most relevant topics in our days. This is the future of transport. They rely on artificial intelligence to recognize and adapt to driving conditions and behavior. This is something we are working on at https://jccbowers.com
7. Facebook and Twitter are two companies that use AI to compare people with relevant content on the relevant sites. Google was one of the first to improve the use of AI to help people find information that they are looking for. Meanwhile, Facebook uses AI to sift through their information to keep them up to date on any alarming issues. Another very well known feature would be facial recognition. It studies and learns facial features to be later able to recognize you in another photo.
8. You can interact with artificial intelligence much more than you think. There are hundreds of millions of them, including personalized assistants on our mobile phones, such as Siri and Cortana. We use them to find time for a movie and a gas station nearby or a weather forecast. We are also working on an AI program to help you connect with your vehicle. Rolling up your windows when it rains, turning the vehicle on and starting the heater before your morning commute or just informing you of an upcoming oil change is a few of the things “Cobie” will be able to do for you.
9. Our future, of course, is intertwined with artificial intelligence. It is inevitable that it will exceed the abilities of man, but most of its skills will also improve our lives.
10. ""Quantum Artificial Intelligence"" will use AI for advanced medications in medicine or mapping an even more significant part of the universe.
About JCC Bowers
JCC Bowers is a UK-based technology solutions provider for Connected, Intelligent, and Autonomous systems for a variety for transportation industries, including automotive, aerospace, nautical, agriculture, and defense. The company’s suite of products enables major existing industry participants to bridge legacy technological shortfalls and address the coming digital convergence through previously unimaginable Connected, Intelligent and Autonomous capabilities. | https://jccbowers.com
14.9k Views · View 56 Upvoters · View Sharers",Miguel Murillo,What are the interesting facts about AI?,Artificial Intelligence
54,"When I was in college, I was fortunate to work with a professor whose first name is Christopher. He goes by Chris, and some of his students occasionally misspell his name into Christ. Once this happened on Twitter, and a random guy replied:
Nail him on the cross… -entropy.
So, cross-entropy clearly made a cool machine learning pun. You should use cross-entropy in your next joke!
Okay, enough jokes. Cross-entropy loss is generally used as a loss function for classification problem. The assumption is that your model predicts a probability distribution
𝑝(𝑦=𝑖)
p
for each class
𝑖=1,2,…,𝐶
i
. If the correct class is c, then the cross-entropy loss is
=−
∑
𝐶
𝑖=1
1[𝑖=𝑐]log𝑝(𝑦=𝑖)=−log𝑝(𝑦=𝑐)
L
This loss works well in practice, meaning that, ignoring whatever maths I wrote below, most of the time, just using this loss function to train your ML model will make it work relatively well.
Now, this cross-entropy loss function has several deeper interpretations. Here is my favorite one.
We assume that for each input to an ML model, there is an optimal distribution of the correct class. That is, for example, if a model sees an image of a car, then even though it should assign a high probability for the class car, it should ideally assign a higher probability to the class horse than to the class snake, because a car looks more like a horse than like a snake. Let’s call this optimal distribution
𝑝
∗
p
.
The purpose of the process of training a machine learning model is to minimize the Kullback-Leibler divergence from
𝑝
∗
p
to
𝑝
p
. The formula is:
=−
∑
𝐶
𝑖=1
𝑝
∗
(𝑦=𝑖)log
𝑝(𝑦=𝑖)
𝑝
∗
(𝑦=𝑖)
L
=−
∑
𝐶
𝑖=1
𝑝
∗
(𝑦=𝑖)log𝑝(𝑦=𝑖)+
∑
𝐶
𝑖=1
𝑝
∗
(𝑦=𝑖)log
𝑝
∗
(𝑦=𝑖)
=
Now because
𝑝
∗
p
has nothing to do with your model, the optimization process does not care about the second term above. Thus, the objective becomes:
=−
∑
𝐶
𝑖=1
𝑝
∗
(𝑦=𝑖)log𝑝(𝑦=𝑖)
L
Unfortunately, this optimal distribution is not given in your training data (it’s too hard and expensive to collect). Thus, people use
𝑝
̂ 
∗
(𝑦=𝑖)=1[𝑖=𝑐]
p
, that is, all the probability mass concentrates into the correct class.
This is not ideal, as Hinton et al (2015) figured out in a technique called knowledge distillation, a better distribution
𝑝
∗
p
can lead to better results, but that is for another story. If you substitute
𝑝
̂ 
∗
p
into the equation above, you will have the cross-entropy loss. You should use cross-entropy loss when you don’t have a better
𝑝
∗
p
. It’s not ideal, but it works reasonably well.
Edit: (also written in a comment). Another interpretation of the cross-entropy loss function.
Another interpretation of cross-entropy is from reinforcement learning. Assume that you sample the correct class from your model’s distribution, similar to the way you would sample an action from a reinforcement learning policy. Your correct “action”, i.e. the correct class, gets you a reward of 1, while other actions gets you a reward of 0.
The REINFORCE equation says that your gradient with respect to the expected reward is
∇
𝜃
𝔼
𝑎∼𝑝(𝑎;𝜃)
[𝑅(𝑎)]=
𝔼
𝑎∼𝑝(𝑎;𝜃)
[𝑅(𝑎)
∇
𝜃
log𝑝(𝑎;𝜃)]
The full evaluation of the expectation on the right-hand side gives you the negative cross-entropy loss. However, in RL, you maximize the expected reward, so you get the idea ^_^
This interpretation suggests that you can approximate the cross-entropy loss the same way you would do Monte-Carlo approximation in RL. This approach is extremely helpful when the number of your classes is large, e.g. the vocabulary for neural machine translation (NMT). In fact, this has been done in the context of language model, under the name noise contrastive estimation, as well as in the context of neural machine translation .
Another possible improvement from RL is that you can somehow apply actor-critic. I’m working on this, so please do not scoop me. I’m just a poor grad student…
Yet, another possible improvement is that you can also improve your “reward”, because 0/1 seems very crude.
11.6k Views · View 112 Upvoters · View Sharers",Hieu Pham,When should you use cross entropy loss and why?,Artificial Intelligence
55,"To answer this question, lets revisit the components of an MDP, the most typical decision making framework for RL.
An MDP is typically defined by a 4-tuple
(𝑆,𝐴,𝑅,𝑇)
(
where
𝑆
S
is the state/observation space of an environment
𝐴
A
is the set of actions the agent can choose between
𝑅(𝑠,𝑎)
R
is a function that returns the reward received for taking action
𝑎
a
in state
𝑠
s

𝑇(
𝑠
′
|𝑠,𝑎)
T
is a transition probability function, specifying the probability that the environment will transition to state
𝑠
′
s
if the agent takes action
𝑎
a
in state
𝑠
s
.
Our goal is to find a policy
𝜋
π
that maximizes the expected future (discounted) reward.
Now if we know what all those elements of an MDP are, we can just compute the solution before ever actually executing an action in the environment. In AI, we typically call computing the solution to a decision-making problem before executing an actual decision planning. Some classic planning algorithms for MDPs include Value Iteration, Policy Iteration, and whole lot more.
But the RL problem isn’t so kind to us. What makes a problem an RL problem, rather than a planning problem, is the agent does *not* know all the elements of the MDP, precluding it from being able to plan a solution. Specifically, the agent does not know how the world will change in response to its actions (the transition function
𝑇
T
), nor what immediate reward it will receive for doing so (the reward function
𝑅
R
). The agent will simply have to try taking actions in the environment, observe what happens, and somehow, find a good policy from doing so.
So, if the agent does not know the transition function
𝑇
T
nor the reward function
𝑅
R
, preventing it from planning a solution out, how can it find a good policy? Well, it turns out there are lots of ways!
One approach that might immediately strike you, after framing the problem like this, is for the agent to learn a model of how the environment works from its observations and then plan a solution using that model. That is, if the agent is currently in state
𝑠
1
s
, takes action
𝑎
1
,
a
and then observes the environment transition to state
𝑠
2
s
with reward
𝑟
2
r
, that information can be used to improve its estimate of
𝑇(
𝑠
2
|
𝑠
1
,
𝑎
1
)
T
and
𝑅(
𝑠
1
,
𝑎
1
)
R
, which can be performed using supervised learning approaches. Once the agent has adequately modelled the environment, it can use a planning algorithm with its learned model to find a policy. RL solutions that follow this framework are model-based RL algorithms.
As it turns out though, we don’t have to learn a model of the environment to find a good policy. One of the most classic examples is Q-learning, which directly estimates the optimal Q-values of each action in each state (roughly, the utility of each action in each state), from which a policy may be derived by choosing the action with the highest Q-value in the current state. Actor-critic and policy search methods directly search over policy space to find policies that result in better reward from the environment. Because these approaches do not learn a model of the environment they are called model-free algorithms.
So if you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: after learning, can the agent make predictions about what the next state and reward will be before it takes each action? If it can, then it’s a model-based RL algorithm. if it cannot, it’s a model-free algorithm.
This same idea may also apply to decision-making processes other than MDPs.
29.2k Views · View 206 Upvoters · Answer requested by
Daniel Yamins",James MacGlashan,What is the difference between model-based and model-free reinforcement learning?,Artificial Intelligence
56,"Generally 3 properties are expected from activation function:
nonlinearity - that is crucial property of activation function. Thanks to that neural network can be used to solved nonlinear problems.
continuously differentiable – which means that we have a continuous first order derivative. It is desirable property for enabling gradient-based optimization methods. Activation function which is continuously differentiable does not make any problems for gradient-based optimization methods.
monotonic – it helps the neural network to converge easier into an more precise model.
ReLU is nonlinear and monotonic. But it is not continuously differentiable. Other activation functions, like sigmoid and tanh, have all the 3 properties. So why the ReLU is so popular (and why it works so well)?
To find the answer it is sufficient to compare formulas and shapes of the typical activation function:
A deeper look can give us and idea what are the advantages of ReLU function. Which are:
Activation functions like sigmoid, tanh or softsign suffer from vanishing gradient problem. Both end of these curves are ‘almost-horizontal’. Gradient values at these parts of the curve are very small or have vanished. Because of that the network refuses to learn further or the learning is drastically slow. Rectifier does not suffer from this. However, it has another problem - dying ReLU problem. For argument lower than 0 the gradient vanishing. Neurons which went into that state stop responding to changes in input or error ( simply because gradient is 0, nothing changes ). A solution for that problem are ReLU modification mentioned above (Noisy ReLU, Leaky ReLU, ELU).
Rectifiers are faster. Simply because they involve simpler mathematical operations. They do not require any normalization and exponential computation (such as those required in sigmoid or tanh activation functions). The training of neural network based on ReLU can be faster up to 6 times in comparison to other activation functions ( see http://www.cs.toronto.edu/~fritz...).
References:
What is the purpose of rectifier functions in neural networks?
Introduction to Different Activation Functions for Deep Learning
36.5k Views · View 206 Upvoters · View Sharers",Tomasz Bąk,Why is ReLU the most common activation function used in neural networks?,Artificial Intelligence
57,"This one still haunts me to this day. I was playing the original Assassins Creed on the Xbox 360 and I was on the mission where you are supposed to assassinate King Richard in Arsuf. So, when i loaded the area there was another Altair who was just standing in front of me, basically a mirror image. When I would move, he would, if I stopped he stopped. This was interesting as I had never seen anything like this. Anyways, so I went to attack him, and he attacks at the same time. I stopped attacking because I got damage from it, but clone Altair keeps attacking me. So I went to run from him, and he chased me and kept trying to attack me. So, I got on horseback and galloped to the mission destination. When I got there I could not proceed, and the game crashes.
I then attempted to play the game again, and same thing except he attacked automatically, so I run, and the game crashes. After that I could never play that disc, due to it being too scratched. I did play it again a few months later and beat it without encountering clone Altair.
1.2k Views · View 32 Upvoters",Daniel Blank,"Dear Gamers, while playing, have you ever, in any game, had an encounter with an AI that made you feel unnerved/uneasy, because you thought the AI might be acting outside its code? In a sentient way? Consciously, even? What did it do/say?",Artificial Intelligence
58,"It means “negative log likelihood”.
62 Views · View 2 Upvoters",Loren Peter Lugosch,What is the meaning of 'train NLL' and 'test NLL' in machine learning?,Artificial Intelligence
59,"Let me start with an old one. Do you know this guy?
This handsome Russian is Garry Kasparov, one of the best grandmasters who ever played chess. In 1985, IBM was developing a chess playing AI named ChipTest. This was later named Deep Thought, culminating in the final product Deep Blue.
In 1996, Garry Kasparov (the reigning world champion at the time) agreed to play against Deep Blue in an international 6-match game. Garry Kasparov won that game (3 wins, 2 draws, 1 loss). IBM engineers returned to their labs, churned those machines, and emerged the next year with a new and improved product. Garry Kasparov played Deep Blue, for the second time, in 1997. This time, the AI won by a small margin.
You can say that Deep Blue is just a case of nested, complicated series of if-else's, but I think the idea of an AI that is capable of beating the reigning world champion, in 1997, in chess, is a very advanced technology, even by today’s standards.
Let’s have another example from a very different domain. I am dead sure you know this guy.
Almost any one in the world would recognize a photo (or painting) of Jesus, Steve Jobs, and George Clooney. But I am not about to praise Jobs, but the product he holds in his hand. Yep, that iPhone over there.
When he unveiled the iPhone in 2007, it was a completely revolutionary move. The “smartphones” we all used were like this:
(Image Source: Top 10 best Nokia phones of all time)
The idea of a keypad-less phone, with a large, multi-touch screen with gesture support was completely different from what people used to know. iOS dominated the world of handheld devices, until Android dethroned it in 2008–09. Still, up to this day, you can’t deny that iOS is an innovative piece of software.
My last example would be this. Do you know this beautiful dame?
This photo is taken from her official Twitter Account. You don’t know her? This is Sophia, the chatterbox humanoid robot that have done it all, from addressing the UN General Assembly, to appearing in The Tonight Show with Jimmy Fallon. She talks, and soon will walk, and she expresses 62 different human emotions. She can hold intelligent conversations and maintain eye contact. To top it up, she is the first robot ever to become a citizen (in 2017, Saudi Arabia gave Sophia citizenship[1] [2]).
So I guess that’s my two cents, an AI that mastered chess, an OS that changed how we look at smartphones, and the first robot citizen of the world. Pretty advanced technology, right?
Thanks for A2A.
Footnotes
[1] 5 Most Advanced AI Robots That You Need To See - WixOffers
[2] For the first time ever, a robot was granted citizenship
41.5k Views · View 55 Upvoters · Answer requested by
Z Z",Mohammed Isam,What is the most advanced computer program?,Artificial Intelligence
